#!/usr/bin/env python3

import argparse
import Bio.KEGG.REST as kegg
from collections import OrderedDict
from copy import deepcopy
from functools import partial
from glob import glob
import multiprocessing as mp
import numpy as np
import os
import os.path
import pandas as pd
import pickle as pkl
import seaborn as sb
import subprocess
import sys

blast_table_hdrs = [
    'qseqid', 
    'sseqid', 
    'pident', 
    'length', 
    'mismatch', 
    'gapopen', 
    'qstart', 
    'qend', 
    'sstart', 
    'send', 
    'evalue', 
    'bitscore'
]
ranks = [
    'species', 
    'genus', 
    'family', 
    'order', 
    'class', 
    'phylum', 
    'superkingdom'
]
#All retained bin results
bin_table_hdrs = [
    'qfile', 
    'scan', 
    'seqnum', 
    'peptide', 
    'scans', 
    'speccount', 
    'protlen', 
    'sseqid', 
    'pident', 
    'length', 
    'mismatch', 
    'gapopen', 
    'qstart', 
    'qend', 
    'sstart', 
    'send', 
    'evalue', 
    'bitscore', 
    'genefam', 
    'go', 
    'kegg', 
    'cog', 
    'descrip'
] + ranks
#Non-amino acid characters to remove from peptide strings in MSGF+ output
trans_table = dict.fromkeys(
    map(ord, ''.join(['.', '|', '^', '+', '-'] + [str(i) for i in range(10)])), 
    None
)

#Bitscores are ultimately reported relative to this value, 
#which is the approximate cutoff for a statistically significant alignment (evalue <= 0.01)
sig_cutoff = 27

def main():

    args = get_args()
    global prot_dirs, bin_dir, out_dir, num_cpus
    prot_dirs = args.prot_dirs
    bin_dir = args.bin_dir
    out_dir = args.out
    num_cpus = args.cpu

    #[KEGG dir currently required -- should be optional]
    global kegg_dir
    if args.kegg_dir != None:
        kegg_dir = args.kegg_dir
    else:
        kegg_dir = None

    #Map proteomic dataset to state from user-provided table
    global prot_state_dict, state_prots_dict
    prot_state_dict = OrderedDict()
    state_prots_dict = OrderedDict()
    if args.state != None:
        state_df = pd.read_csv(args.state, sep='\t', header=0)
        for _, row in state_df.iterrows():
            prot_name = row.iloc[0]
            state = row.iloc[1]
            prot_state_dict[prot_name] = state
            try:
                state_prots_dict[state].append(prot_name)
            except KeyError:
                state_prots_dict[state] = [prot_name]
        #List all states specified
        global all_states
        all_states = list(set(state_df.iloc[:, 1].tolist()))

    global manual_group_fp
    if args.group != None:
        manual_group_fp = args.group
    else:
        manual_group_fp = None

    #Predict genes from bin contigs
    run_prodigal()

    #Make BLAST+ database from each bin
    blast_db_fps = []
    #All bins are contained in a single directory
    global bin_names
    bin_names = []
    for bin_basename in os.listdir(bin_dir):
        #List the BLAST+ database filepaths
        blast_db_fps.append(make_blast_db(os.path.join(bin_dir, bin_basename)))
        bin_names.append(os.path.splitext(bin_basename)[0])

    #BLAST PSM ORFs against bins
    query_fasta_fps = []
    global prot_names
    prot_names = []
    for prot_dir in args.prot_dirs:
        #The directory containing a proteomic dataset must have the dataset's name
        prot_name = os.path.normpath(os.path.basename(prot_dir))
        prot_names.append(prot_name)
        #Make a query fasta containing the ORFs corresponding to the peptides
        #Place each bin's fasta in a separate directory
        query_fasta_dir = os.path.join(out_dir, prot_name + '.bin_search')
        query_fasta_fps.append(os.path.join(query_fasta_dir, prot_name + '.blastp_queries.faa'))
        if os.path.exists(query_fasta_fps[-1]):
            print('Query path', query_fasta_fps[-1], 'already exists', flush=True)
        else:
            make_query_fasta(prot_name, prot_dir)

    if prot_state_dict:
        state_pipeline(prot_state_dict, blast_db_fps, query_fasta_fps)
    else:
        stateless_pipeline(blast_db_fps, query_fasta_fps)

    return

def get_args():
    '''
    Get command line arguments
    '''
    
    parser = argparse.ArgumentParser()
    #Each proteomic dataset is contained in a separate directory
    #Suggestion: make a separate file listing the directories on each line and expand with cat: 
    #$(cat ~/prot_dirs.txt | tr '\n' ' ')
    parser.add_argument(
        '-p', 
        '--prot_dirs', 
        nargs='+', 
        help='List of directories for each proteomic dataset'
    )
    parser.add_argument(
        '-b', 
        '--bin_dir', 
        help='Directory exclusively containing bin fastas'
    )
    parser.add_argument(
        '-o', 
        '--out', 
        help='Output directory'
    )
    #KEGG files generated by ec_retrieval which link KO, EC, and Pathway Maps
    #[Change protein-express to incorporate ec_retrieval and automate updating central files]
    parser.add_argument(
        '-k', 
        '--kegg_dir', 
        help='Directory containing ko_ec.tsv, ec_map.tsv, map_name.tsv'
    )
    #States are classes of samples (e.g., samples from the same environment)
    parser.add_argument(
        '-s', 
        '--state', 
        help='Table relating each proteome name to a state'
    )
    #Grouping of proteins into user-defined functional groups requires this table
    parser.add_argument(
        '-g', 
        '--group', 
        help='Table relating gene family names or eggNOG descriptions to groups'
    )
    #Prodigal and BLAST can run in parallel
    parser.add_argument(
        '-c', 
        '--cpu', 
        type=int, 
        default=1, 
        help='Number of available CPUs'
    )

    args = parser.parse_args()

    return args

def run_prodigal():
    '''
    Wrapper function for parallel execution of Prodigal
    '''

    fasta_basenames = os.listdir(bin_dir)
    fasta_fps = [os.path.join(bin_dir, fasta_basename) for fasta_basename in fasta_basenames]

    mp_pool = mp.Pool(num_cpus)
    mp_pool.map(prodigal_worker, contig_fps)
    mp_pool.close()
    mp_pool.join()

    return

def prodigal_worker(contig_fp):
    '''
    Predict ORFs from bin contigs
    '''

    contig_dir = os.path.dirname(contig_fp)
    bin_name = os.path.splitext(os.path.basename(contig_fp))[0]
    #Gene start positions in contigs
    gene_coords_fp = os.path.join(out_dir, bin_name + '.gene_coords.gbk')
    gene_fp = os.path.join(out_dir, bin_name + '.faa')

    if not os.path.exists(gene_fp):
        fnull = open(os.devnull, 'w')
        subprocess.call([
            'prodigal', 
            '-i', bin_fp, 
            '-o', gene_coords_fp, 
            '-a', gene_fp
        ], stdout=fnull, stderr=fnull)
        fnull.close()
    else:
        print('prodigal output', gene_fp, 'already exists', flush=True)        
        
    return

def make_blast_db(bin_fp):
    '''
    Make blast database from proteins in fasta
    '''

    bin_dir = os.path.dirname(bin_fp)
    bin_name = os.path.splitext(os.path.basename(bin_fp))[0]

    # Make a blast database from the proteins
    blast_db_dir = os.path.join(out_dir, bin_name + '.blast_db')
    blast_db_fp = os.path.join(blast_db_dir, bin_name)
    proteins_fp = os.path.join(out_dir, bin_name + '.faa')
    try:
        os.mkdir(blast_db_dir)
        subprocess.call([
            'makeblastdb', 
            '-dbtype', 'prot', 
            '-in', proteins_fp, 
            '-out', blast_db_fp, 
            '-hash_index'
        ])         
    except FileExistsError:
        print('blast database', blast_db_fp, 'already exists', flush=True)

    return blast_db_fp    

def make_query_fasta(prot_name, prot_dir):
    '''
    Retrieve PSM ORF sequences for search against bins
    '''

    #Load the table of unique peptides reported by Postnovo
    peptide_df = pd.read_csv(
        os.path.join(prot_dir, 'reported_df.tsv'), 
        sep='\t', 
        header=0
    )
    scans = peptide_df['scan_list'].tolist()
    #Dataset (e.g., metagenome) origins of unique ORFs containing PSMs
    best_predict_origins = peptide_df['best_predicts_from'].tolist()
    best_predict_origins = [s.split(',') for s in best_predict_origins]
    #Origins of non-unique ORFs with PSMs that are subseqs of unique ORFs
    redun_predict_origins = peptide_df['also_contains_predicts_from'].tolist()
    redun_predict_origins = [
        [''] if pd.isnull(s) else s.split(',') for s in redun_predict_origins
    ]

    predict_origins = []
    # Remove empty strings from origin lists
    for i, best_predict_origin in enumerate(best_predict_origins):
        l = []
        predict_origins.append(l)

        m = best_predict_origin
        if m == ['']:
            pass
        elif '' in m:
            l += m.remove('')
        else:
            l += m

        m = redun_predict_origins[i]
        if m == ['']:
            pass
        elif '' in m:
            l += m.remove('')
        else:
            l += m

    #Get the names of all origin datasets
    fasta_fps = glob(
        os.path.join(prot_dir, prot_name + '.*.reads.fasta')
    )
    fasta_fps += glob(
        os.path.join(prot_dir, prot_name + '.*.DBGraphPep2Pro.fasta')
    )
    #Record the ORF seqs and ids from the origin fastas
    fasta_ids = OrderedDict()
    fasta_seqs = OrderedDict()
    for fasta_fp in fasta_fps:
        ref_name = os.path.basename(fasta_fp).replace(prot_name + '.', '').replace('.fasta', '')
        with open(fasta_fp) as handle:
            lines = handle.readlines()
        fasta_ids[ref_name] = [line.lstrip('>').split(' ')[0] for line in lines[::2]]
        fasta_seqs[ref_name] = [line.rstrip() for line in lines[1::2]]
        assert len(fasta_ids[ref_name]) == len(fasta_seqs[ref_name]), \
        fasta_fp + ' does not have an even number of lines'

    #Record the scans, peptides, and ORF seqs from each file of db search results
    db_search_scans = OrderedDict().fromkeys(fasta_ids)
    db_search_peps = OrderedDict().fromkeys(fasta_ids)
    db_search_hits = OrderedDict().fromkeys(fasta_ids)
    for ref_name in fasta_ids:
        if '.reads' in ref_name:
            db_search_basename = prot_name + '.' + ref_name + \
            '.graph2pro_derep.fgs.tryptic.derep.0.01.tsv'
        elif '.DBGraphPep2Pro' in ref_name:
            db_search_basename = prot_name + '.' + ref_name + '.fixedKR.0.01.tsv'
        db_search_fp = os.path.join(prot_dir, db_search_basename)
        db_search_df = pd.read_csv(db_search_fp, sep='\t', header=0)
        db_search_scans[ref_name] = db_search_df['ScanNum'].tolist()
        db_search_peps[ref_name] = db_search_df['Peptide'].tolist()
        l = []
        db_search_hits[ref_name] = l
        for hits_str in db_search_df['Protein'].tolist():
            l.append([
                hit_str[:hit_str.index('(pre=')] for hit_str in hits_str.split(';')
            ])

    first_scans = [int(scans_str.split(',')[0]) for scans_str in scans]
    #It is possible that a spectrum may have been ID'd as different precursor peptide sequences
    peps = OrderedDict([(first_scan, []) for first_scan in first_scans])
    orfs = OrderedDict([(first_scan, []) for first_scan in first_scans])
    orf_origins = OrderedDict([(first_scan, []) for first_scan in first_scans])
    for i, scan in enumerate(first_scans):
        #List of non-redundant ORFs matching the scan
        peps_scan = peps[scan]
        orfs_scan = orfs[scan]
        orf_origins_scan = orf_origins[scan]
        best_predict_origins_scan = best_predict_origins[i]
        for ref_name in best_predict_origins_scan:
            #The spectrum (scan) may not have matched the ref db
            try:
                j = db_search_scans[ref_name].index(scan)
                db_search_ref_peps = db_search_peps[ref_name]
                for hit in db_search_hits[ref_name][j]:
                    #[Why did I include the next exception? I don't think it occurs]
                    try:
                        k = fasta_ids[ref_name].index(hit)
                        orf = fasta_seqs[ref_name][k]
                        #The ORF may have already been recorded from a different ref db
                        try:
                            x = orfs_scan.index(orf)
                            orf_origins_scan[x].append(ref_name)
                        except ValueError:
                            #Assume that peptides are the same if ORFs are the same
                            peps_scan.append(db_search_ref_peps[j])
                            orfs_scan.append(orf)
                            orf_origins_scan.append([ref_name])
                    except ValueError:
                        pass
            except (KeyError, ValueError):
                pass

    search_dir = os.path.join(out_dir, prot_name + '.bin_search')
    subprocess.call(['mkdir', '-p', search_dir])

    #Store a file mapping scan ID and ORF sequence number to peptide seq
    peps_fp = os.path.join(search_dir, prot_name + '.peps.pkl')
    with open(peps_fp, 'wb') as handle:
        pkl.dump(peps, handle, 2)

    query_fasta = []
    for first_scan in first_scans:
        orfs_scan = orfs[first_scan]
        if orfs_scan:
            for i, orf in enumerate(orfs_scan):
                #Index the ORFs containing the same peptide
                seq_id = '>' + str(first_scan) + '.' + str(i) + '\n'
                query_fasta.append(seq_id)
                query_fasta.append(orf + '\n')

    query_fasta_fp = os.path.join(search_dir, prot_name + '.blastp_queries.faa')
    with open(query_fasta_fp, 'w') as handle:
        for line in query_fasta:
            handle.write(line)
            
    return

def state_pipeline(prot_state_dict, blast_db_fps, query_fasta_fps):
    '''
    Compare proteomic datasets that have different states
    '''

    expanded_annots = False
    if expanded_annots:
        postnovo_table_f = 'reported_df.tsv'
    else:
        postnovo_table_f = 'reported_df.old_annot.tsv'

    #Map states to lists of bin results
    state_bin_table_fps_dict = OrderedDict([(state, []) for state in all_states])
    bin_table_fps = []
    for i, bin_name in enumerate(bin_names):
        blast_db_fp = blast_db_fps[i]
        bin_table_fps_for_bin = []
        for state in all_states:
            #Filepath of BLAST+ output for bin
            bin_table_fp = os.path.join(out_dir, bin_name + '.' + state + '.blast_out.txt')
            state_bin_table_fps_dict[state].append(bin_table_fp)
            bin_table_fps_for_bin.append(bin_table_fp)
        bin_table_fps += bin_table_fps_for_bin
        #BLAST each set of ORFs against bin fasta
        for prot_name, query_fasta_fp, prot_dir in zip(prot_names, query_fasta_fps, prot_dirs):
            state = prot_state_dict[prot_name]
            #Create directories for each proteomic dataset to store bin search results
            search_dir = os.path.join(out_dir, prot_name + '.bin_search')
            blast_table_fp = os.path.join(
                search_dir, prot_name + '.' + bin_name + '.blastp_hits.out'
            )
            if os.path.exists(blast_table_fp):
                print(blast_table_fp, 'already exists', flush=True)
            else:
                run_blastp(blast_db_fp, query_fasta_fp, blast_table_fp)
            #Manually Expanded annotations can be used (for all but EC + Gene Family).
            #For example, a peptide may receive a Description of rubrerythrin, but not the Gene Family, 
            #so such entries are assigned the Gene Family annotation of RBR.
            postnovo_table_fp = os.path.join(prot_dir, postnovo_table_f)
            #File containing unique peptide sequences
            peps_fp = os.path.join(search_dir, prot_name + '.peps.pkl')
            bin_table_fp = os.path.join(out_dir, bin_name + '.' + state + '.blast_out.txt')
            #UNCOMMENT
            parse_blast_table(
                prot_name, 
                blast_table_fp, 
                blast_db_fp, 
                postnovo_table_fp, 
                peps_fp, 
                bin_table_fp
            )
        #Since different proteomic datasets can contain spectra from the same peptides, 
        #remove information regarding redundant alignments to the bin sequences
        for bin_table_fp in bin_table_fps_for_bin:
            remove_redun_peps(bin_table_fp)

    #Compare bins using a choice of functional annotations:
    #Gene Family, GO, EC (from KO)
    #Pathway Map can be specified in addition to EC
    #The following combinations of annotations can be used:
    #EC + Gene Family for peptides without EC
    #Gene Family + eggNOG Description for peptides without Gene Family -- 
    #Due to the relatively high variability of Description strings, 
    #functional categories are manually defined in advance 
    #by sets of Gene Families and Descriptions. 
    #These categories are relatively fine-grained and focused on biogeochemical functions.
    annot_compar_table_fp_dict = OrderedDict(
        [(annot_method, OrderedDict()) for annot_method in 
         ['genefam', 'go', 'ec', 'map', 'genefam+descrip', 'ec+genefam']
        ]
    )
    sys_dict = systematize_genefams(postnovo_table_f)
    man_group_dict = load_man_groups()
    for state, bin_table_fps in state_bin_table_fps_dict.items():
        #Gene Family
        annot_compar_table_fp_dict['genefam'][state] = compare_bins(
            'genefam', bin_table_fps, state=state
        )
        #GO
        annot_compar_table_fp_dict['go'][state] = compare_bins(
            'go', bin_table_fps, state=state
        )
        #EC, Pathway Map
        if kegg_dir != None:
            annot_compar_table_fp_dict['ec'][state], annot_compar_table_fp_dict['map'][state] = \
                compare_bins('ec+map', bin_table_fps, state=state)
        #Gene Family + Description
        #Manual functional group definitions
        #Modify the unsystematized bin tables with the systematized annotations
        sys_bin_table_fps = systematize_bin_tables(bin_table_fps, sys_dict)
        annot_compar_table_fp_dict['genefam+descrip'][state], \
            annot_compar_table_fp_dict['manual_group'][state] = compare_bins(
                'genefam+descrip', sys_bin_table_fps, man_group_dict=man_group_dict, state=state
            )
        #EC + Gene Family
        if kegg_dir != None:
            annot_compar_table_fp_dict['ec+genefam'][state] = compare_bins(
                'ec+genefam', bin_table_fps, state=state
            )

    #state_compar_table_fp['tussock'] = '/scratch/samuelmiller/12-26-17/postnovo/io/toolik/protein-express_out/compar_table.tussock.tsv'
    #state_compar_table_fp['intertussock'] = '/scratch/samuelmiller/12-26-17/postnovo/io/toolik/protein-express_out/compar_table.intertussock.tsv'
    #state_compar_table_fp['shrub'] = '/scratch/samuelmiller/12-26-17/postnovo/io/toolik/protein-express_out/compar_table.shrub.tsv'
    for annot_method, state_compar_table_fp_dict in annot_compar_table_fp_dict.items():
        compare_states(state_compar_table_fp_dict, annot_method)

    #Compute NSAF statistics for proteomic datasets and states
    #NSAF statistics of metaproteins defined by EC ID are output in a format for plotting in Vanted
    nsaf_analysis(prot_state_dict, postnovo_table_f, sys_dict['genefam_descrip'], man_group_dict)

    return

def stateless_pipeline(blast_db_fps, query_fasta_fps):
    '''
    Compare proteomic datasets that were not assigned to states
    '''

    expanded_annots = False
    if expanded_annots:
        postnovo_table_f = 'reported_df.tsv'
    else:
        postnovo_table_f = 'reported_df.old_annot.tsv'

    bin_table_fps = []
    for i, bin_name in enumerate(bin_names):
        blast_db_fp = blast_db_fps[i]
        bin_table_fp = os.path.join(out_dir, bin_name + '.blast_out.txt')
        bin_table_fps.append(bin_table_fp)

        #BLAST each set of ORFs against bin fasta
        for prot_name, query_fasta_fp, prot_dir in zip(prot_names, query_fasta_fps, prot_dirs):
            #Create directories for each proteomic dataset to store bin search results
            search_dir = os.path.join(out_dir, prot_name + '.bin_search')
            blast_table_fp = os.path.join(
                search_dir, prot_name + '.' + bin_name + '.blastp_hits.out'
            )
            if os.path.exists(blast_table_fp):
                print(blast_table_fp, 'already exists', flush=True)
            else:
                run_blastp(blast_db_fp, query_fasta_fp, blast_table_fp)
            #Manually Expanded annotations can be used (for all but EC + Gene Family).
            #For example, a peptide may receive a Description of rubrerythrin, but not the Gene Family, 
            #so such entries are assigned the Gene Family annotation of RBR.
            postnovo_table_fp = os.path.join(prot_dir, postnovo_table_f)
            #File containing unique peptide sequences
            peps_fp = os.path.join(search_dir, prot_name + '.peps.pkl')
            bin_table_fp = os.path.join(out_dir, bin_name + '.' + state + '.blast_out.txt')
            #UNCOMMENT
            parse_blast_table(
                prot_name, 
                blast_table_fp, 
                blast_db_fp, 
                postnovo_table_fp, 
                peps_fp, 
                bin_table_fp
            )
        #Since different proteomic datasets can contain spectra from the same peptides, 
        #remove information regarding redundant alignments to the bin sequences
        remove_redun_peps(bin_table_fps[-1])

    #Compare bins using a choice of functional annotations:
    #Gene Family, GO, EC (from KO)
    #Pathway Map can be specified in addition to EC
    #The following combinations of annotations can be used:
    #EC + Gene Family for peptides without EC
    #Gene Family + eggNOG Description for peptides without Gene Family -- 
    #Due to the relatively high variability of Description strings, 
    #functional categories are manually defined in advance 
    #by sets of Gene Families and Descriptions. 
    #These categories are relatively fine-grained and focused on biogeochemical functions.

    annot_compar_table_fp_dict = OrderedDict(
        [(annot_method, OrderedDict()) for annot_method in 
         ['genefam', 'go', 'ec', 'map', 'genefam+descrip', 'ec+genefam']
        ]
    )
    sys_dict = systematize_genefams(postnovo_table_f)
    man_group_dict = load_man_groups()
    #Gene Family
    annot_compar_table_fp_dict['genefam'] = compare_bins(
        'genefam', bin_table_fps
    )
    #GO
    annot_compar_table_fp_dict['go'] = compare_bins(
        'go', bin_table_fps
    )
    #EC, Pathway Map
    if kegg_dir != None:
        annot_compar_table_fp_dict['ec'], annot_compar_table_fp_dict['map'] = \
            compare_bins('ec+map', bin_table_fps)
    #Gene Family + Description
    #Manual functional group definitions
    #Modify the unsystematized bin tables with the systematized annotations
    sys_bin_table_fps = systematize_bin_tables(bin_table_fps, sys_dict)
    annot_compar_table_fp_dict['genefam+descrip'], \
        annot_compar_table_fp_dict['manual_group'] = compare_bins(
            'genefam+descrip', sys_bin_table_fps, man_group_dict=man_group_dict
        )
    #EC + Gene Family
    if kegg_dir != None:
        annot_compar_table_fp_dict['ec+genefam'] = compare_bins(
            'ec+genefam', bin_table_fps
        )

    #Compute NSAF statistics for proteomic datasets and states
    #NSAF statistics of metaproteins defined by EC ID are output in a format for plotting in Vanted
    global prot_state_dict, all_states
    prot_state_dict = OrderedDict()
    all_states = []
    for prot_name in prot_names:
        prot_state_dict[prot_name] = prot_name
        all_states.append(prot_name)
    nsaf_analysis(prot_state_dict, postnovo_table_f, sys_dict['genefam_descrip'], man_group_dict)

    return

def run_blastp(blast_db_fp, query_fasta_fp, blast_table_fp):
    '''
    Wrapper for running BLAST+ in parallel
    '''

    print(
        'Aligning', os.path.basename(query_fasta_fp).replace('.blastp_queries.faa', ''), 
        'to', os.path.basename(blast_db_fp), 
        flush=True
    )

    #Query fasta broken into temporary files for parallelization
    tmp_dir = os.path.join(os.path.dirname(query_fasta_fp), 'tmp')
    query_name = os.path.splitext(os.path.basename(query_fasta_fp))[0]
    subprocess.call(['mkdir', tmp_dir])

    with open(query_fasta_fp) as handle:
        fasta = handle.readlines()

    #Split the query file into files for each CPU
    split_size = len(fasta) // num_cpus + ((len(fasta) // num_cpus) % 2)
    file_num = 0
    handle = open(os.path.join(tmp_dir, query_name + '.1.faa'), 'w')
    split_fasta_fps = []
    for i, line in enumerate(fasta):
        if i % split_size == 0 and file_num < num_cpus:
            file_num += 1
            split_fasta_fp = os.path.join(tmp_dir, query_name + '.' + str(file_num) + '.faa')
            split_fasta_fps.append(split_fasta_fp)
            handle.close()
            handle = open(split_fasta_fp, 'w')
        handle.write(line)
    handle.close()
    mp_pool = mp.Pool(num_cpus)
    mp_pool.map(partial(blastp_worker, blast_db_fp=blast_db_fp), split_fasta_fps)
    mp_pool.close()
    mp_pool.join()

    #Merge BLAST tables
    blast_df = pd.DataFrame()
    for split_fasta_fp in split_fasta_fps:
        split_blast_table_fp = os.path.splitext(split_fasta_fp)[0] + '.out'
        try:
            split_blast_df = pd.read_csv(split_blast_table_fp, sep='\t', header=None)
            blast_df = pd.concat([blast_df, split_blast_df])
        #Possible that no peptides in a query file returned significant alignments
        except pd.io.common.EmptyDataError:
            pass
    blast_df.to_csv(blast_table_fp, sep='\t', index=False, header=False)

    subprocess.call(['rm', '-r', tmp_dir])

    return

def blastp_worker(query_fasta_fp, blast_db_fp):
    '''
    Align proteomic dataset ORFs against bin fasta 
    '''

    split_blast_table_fp = os.path.splitext(query_fasta_fp)[0] + '.out'

    subprocess.call([
        'blastp', 
        '-db', blast_db_fp, 
        '-query', query_fasta_fp, 
        '-out', split_blast_table_fp, 
        '-evalue', '0.01', 
        '-outfmt', '6', 
        '-comp_based_stats', '0'
    ])

    return

def parse_blast_table(prot_name, out_fp, blast_db_fp, postnovo_table_fp, peps_fp, bin_table_fp):
    '''
    Add BLAST table to the merged table storing information for all searches against bin
    '''

    blast_df = pd.read_csv(out_fp, sep='\t', names=blast_table_hdrs, dtype={'qseqid': str})
    #Each ORF is ID'd by a unique scan number-sequence number pair
    blast_df['scan'] = blast_df['qseqid'].apply(lambda s: int(s.split('.')[0]))
    blast_df['seqnum'] = blast_df['qseqid'].apply(lambda s: int(s.split('.')[1]))
    blast_df.drop('qseqid', axis=1, inplace=True)
    #For each peptide, retain the ORF with the lowest evalue
    blast_df = blast_df[blast_df.groupby('scan')['evalue'].transform(min) == blast_df['evalue']]
    blast_df = blast_df.groupby('scan', as_index=False).first()
    blast_df.sort_values('scan', inplace=True)

    postnovo_df = pd.read_csv(postnovo_table_fp, dtype={'scan count': str}, sep='\t', header=0)
    #Each peptide is ID'd by the first scan in the scan list
    postnovo_df['scan'] = postnovo_df['scan_list'].apply(lambda s: int(s.split(',')[0]))
    postnovo_df.sort_values('scan', inplace=True)
    postnovo_df.set_index('scan', inplace=True)
    #Retain peptides with significant alignments to the bin under consideration
    postnovo_df = postnovo_df.loc[blast_df['scan'].tolist()].reset_index()
    #Information imported from Postnovo table, functional annotations from eggNOG-mapper
    postnovo_df = postnovo_df[
        [
            'scan', 
            'scan_list', 
            'scan count', 
            'protein length', 
            'predicted name', 
            'go terms', 
            'kegg pathways', 
            'cog cat', 
            'eggnog hmm desc'
        ] + ranks
    ]
    postnovo_df.rename(
        columns={
            'scan_list': 'scans', 
            'scan count': 'speccount', 
            'protein length': 'protlen', 
            'predicted name': 'protein', 
            'go terms': 'go', 
            'kegg pathways': 'kegg', 
            'cog cat': 'cog', 
            'eggnog hmm desc': 'descrip'
        }, 
        inplace=True
    )
    assert len(blast_df) == len(postnovo_df)
    merged_df = blast_df.merge(postnovo_df, on='scan')
    del(blast_df)

    #Load dictionary mapping scan ID and ORF sequence number to peptide sequence
    #Again, the same spectrum can occasionally match different peptide sequences, 
    #so the additional ORF sequence number is important
    with open(peps_fp, 'rb') as handle:
        peps = pkl.load(handle)
    orf_peps = []
    seq_nums = merged_df['seqnum'].tolist()
    for scan, seq_num in zip(merged_df['scan'].tolist(), seq_nums):
        orf_peps.append(peps[scan][seq_num].translate(trans_table))
    merged_df['peptide'] = orf_peps

    merged_df['scan'] = merged_df['scan'].apply(str)

    #Load the bin table if it exists, else make a new one
    try:
        bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)
    except FileNotFoundError:
        bin_df = pd.DataFrame(columns=bin_table_hdrs)

    # Remove any prior entries for the proteomic dataset under consideration
    if prot_name in bin_df['qfile'].tolist():
        bin_df = bin_df[bin_df['qfile'] != prot_name]
    merged_df['qfile'] = prot_name
    bin_df = pd.concat([bin_df, merged_df], ignore_index=True)
    bin_df = bin_df[bin_table_hdrs]
    bin_df.to_csv(bin_table_fp, sep='\t', index=False)

    return bin_table_fp

def remove_redun_peps(bin_table_fp):
    '''
    Remove redundant peptide alignments to the bin
    '''

    bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)
    bin_df = bin_df.groupby('peptide', as_index=False).first()
    bin_df = bin_df[bin_table_hdrs]
    bin_df.sort_values(['qfile', 'scan'], inplace=True)
    bin_df.to_csv(bin_table_fp, sep='\t', index=False)

    return

def make_sys_postnovo_tables(postnovo_table_f, genefam_descrip_dict, man_group_dict):

    genefam_group_dict = man_group_dict['genefam']
    descrip_group_dict = man_group_dict['descrip']
    prot_sys_postnovo_df_dict = OrderedDict()
    for prot_name, prot_dir in zip(prot_names, prot_dirs):
        postnovo_table_fp = os.path.join(prot_dir, postnovo_table_f)
        postnovo_df = pd.read_csv(postnovo_table_fp, sep='\t', header=0)[
            'predicted name', 'eggnog hmm desc'
        ].rename(
            dict([('predicted name', 'genefam'), ('eggnog hmm desc', 'descrip')])
        ).fillna('')
        genefams = postnovo_df['genefam'].tolist()
        old_descrips = postnovo_df['descrip'].tolist()
        new_descrips = []
        for genefam, descrip in zip(genefams, old_descrips):
            if genefam == '':
                new_descrips.append(descrip)
            else:
                new_descrips.append(genefam_descrip_dict[genefam])
        postnovo_df['descrip'] = new_descrips
        #Add column of manual group annotations based on Gene Family and Description annotations
        groups = []
        for genefam, descrip in zip(genefams, new_descrips):
            if genefam == '':
                try:
                    groups.append(descrip_group_dict['descrip'])
                except KeyError:
                    groups.append('')
            else:
                try:
                    groups.append(genefam_group_dict['genefam'])
                except KeyError:
                    groups.append('')
        postnovo_df['group'] = groups
        prot_sys_postnovo_df_dict[prot_name] = postnovo_df

    return prot_sys_postnovo_df_dict

def systematize_genefams(postnovo_table_f):
    '''
    Curate functional annotations, 
    ensuring that Gene Families map to consistent eggNOG Descriptions and COGs.
    For peptides without Gene Family annotation, ensure Descriptions map to consistent COGs.
    '''

    #Systematize annotations based on ALL samples' annotations
    postnovo_df = pd.DataFrame(columns=['protein', 'eggnog hmm desc', 'cog cat'])
    for prot_name, prot_dir in zip(prot_names, prot_dirs):
        postnovo_table_fp = os.path.join(prot_dir, postnovo_table_f)
        prot_postnovo_df = pd.read_csv(postnovo_table_fp, sep='\t', header=0)[
            ['protein', 'eggnog hmm desc', 'cog cat']
        ]
        postnovo_df = pd.concat([postnovo_df, prot_postnovo_df])
    postnovo_df.rename(dict([
        ('protein', 'genefam'), ('eggnog hmm desc', 'descrip'), ('cog cat', 'cat')
        ]), inplace=True)
    postnovo_df.fillna('', inplace=True)

    #The Description and COG category assigned to each Gene Family
    #are the Description and COG of a majority of peptides with the Gene Family annotation.
    #The COG assigned to each Description without a Gene Family
    #is the COG of a majority of peptides with the Description.
    genefam_descrip_count_dict = dict()
    genefam_cog_count_dict = dict()
    descrip_cog_count_dict = dict()
    genefams = postnovo_df['genefam'].tolist()
    descrips = postnovo_df['descrip'].tolist()
    cogs = postnovo_df['cog'].tolist()
    del(postnovo_df)

    for genefam, descrip, cog in zip(genefams, descrips, cogs):
        if genefam != '':
            #Considering a peptide with a Gene Family annotation
            #Record counts for the full Gene Family entry, 
            #although the entry may be a list of multiple Gene Families
            try:
                #Count each Description associated with the Gene Family entry
                d = genefam_descrip_count_dict[genefam]
                try:
                    d[descrip] += 1
                except KeyError:
                    d[descrip] = 1 
            except KeyError:
                #Add a Description count dict for a Gene Family not in the parent dict
                d = genefam_descrip_count_dict[genefam] = dict()
                d[descrip] = 1

            try:
                #Count each COG associated with the Gene Family entry
                d = genefam_cog_count_dict[genefam]
                try:
                    d[cog] += 1
                except KeyError:
                    d[cog] = 1
            except KeyError:
                #Add a COG category count dict for a Gene Family not in the parent dict
                d = genefam_cog_count_dict[genefam] = dict()
                d[cog] = 1

        try:
            #Count each COG category associated with a peptide Description
            d = descrip_cog_count_dict[descrip]
            try:
                d[cog] += 1
            except KeyError:
                d[cog] = 1
        except KeyError:
            #Add a COG category count dict for a Description not in the parent dict
            d = descrip_cog_count_dict[descrip] = dict()
            d[cog] = 1

    sys_dict = OrderedDict([
        ('genefam_descrip', OrderedDict()), 
        ('genefam_cog', OrderedDict()), 
        ('descrip_cog', OrderedDict())
    ])
    genefam_descrip_dict = sys_dict['genefam_descrip']
    genefam_cog_dict = sys_dict['genefam_cog']
    descrip_cog_dict = sys_dict['descrip_cog']
    #Determine the majority Description for Gene Family annotation
    for genefam, descrip_count_dict in genefam_descrip_count_dict.items():
        max_count = 0
        best_descrip = ''
        #Loop through each Description associated with the Gene Family
        for descrip, count in descrip_count_dict.items():
            if count > max_count:
                best_descrip = descrip
        genefam_descrip_dict[genefam] = best_descrip
    #Determine the majority COG category for Gene Family annotation
    for genefam, cog_count_dict in genefam_cog_count_dict.items():
        max_count = 0
        best_cog = ''
        for cog, count in cog_count_dict.items():
            if count > max_count:
                best_cog = cog
        genefam_cog_dict[genefam] = best_cog
    #Determine the majority COG category for Description
    for descrip, cog_count_dict in descrip_cog_count_dict.items():
        max_count = 0
        best_cog = ''
        for cog, count in cog_count_dict.items():
            if count > max_count:
                best_cog = cog
        descrip_cog_dict[descrip] = best_cog

    return sys_dict

def load_man_groups():

    man_group_df = pd.read_csv(
        manual_group_fp, sep='\t', header=0, names=['group', 'genefam', 'descrip']
    ).fillna('')
    man_group_dict = OrderedDict([
        ('genefam', OrderedDict([
            (genefam, group) for genefam, group 
            in zip(man_group_df['genefam'].tolist(), man_group_df['group'].tolist()) 
            if genefam != ''
        ])), 
        ('descrip', OrderedDict([
            (genefam, group) for genefam, group 
            in zip(man_group_df['genefam'].tolist(), man_group_df['group'].tolist()) 
            if genefam != ''
        ]))
    ])

    return man_group_dict

def compare_bins(option, bin_table_fps, man_group_dict=None, state=None, adjust=False):
    '''
    Wrapper for different bin comparison methods
    '''

    #Relatively direct method of functional annotation
    if option == 'genefam':
        compar_table_fps = compare_bins_single_annot('genefam', bin_table_fps, state)
    elif option == 'go':
        compar_table_fps = compare_bins_single_annot('go', bin_table_fps, state)
    #KO IDs are translated to EC IDs and optionally Pathway Maps
    elif option == 'ec':
        compar_table_fps = compare_bins_ec(bin_table_fps, state)
    elif option == 'ec+map':
        compar_table_fps = compare_bins_ec(bin_table_fps, state, path_map=True)
    #Two annotations: annotation 1 followed by annotation 2 for peptides without 1
    elif option == 'ec+genefam':
        compar_table_fps = compare_bins_ec_genefam(bin_table_fps, state)
    #Manually curated functional categories of Gene Families and Descriptions
    elif option == 'genefam+descrip':
        compar_table_fps = compare_bins_genefam_descrip(bin_table_fps, man_group_dict, state)

    return compar_table_fps

def compare_bins_single_annot(annot, bin_table_fps, state=None):
    '''
    Compare bins using one of Gene Family or GO as functional annotation
    '''

    compar_df = pd.DataFrame(columns=[annot])
    for i, bin_table_fp in enumerate(bin_table_fps):
        bin_name = bin_names[i]
        bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)[
            [annot,  'bitscore'] + ranks
        ]
        #Drop peptides lacking annotation under consideration
        bin_df[annot] = bin_df[pd.notnull(bin_df[annot])]

        #Replicate rows for each annotation with multiple values, splitting the compound annotation
        new_rows = []
        for t in bin_df.itertuples():
            for a in t[1].split(','):
                new_rows.append(tuple([a]) + t[2:])
        bin_df = pd.DataFrame(new_rows, columns=bin_df.columns)

        #Make a table of count statistics for all peptides sharing annotation under consideration
        annot_gb = bin_df.groupby(annot)
        annot_df = annot_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
        annot_df[annot] = [a for a, _ in annot_gb]
        annot_df.reset_index(inplace=True, drop=True)
        #Find consensus (LCA) taxonomic ranks for peptides sharing annotation
        annot_df[ranks] = merge_ranks(annot_gb)

        #Summarize information from each bin in a table containing a block of columns for each bin
        #Sort columns
        bin_summary_df = annot_df[[annot, 'mean', 'min', 'max', 'count'] + ranks]
        bin_summary_df['mean'] = bin_summary_df['mean'].round(1)
        bin_summary_df.rename(
            columns=dict(
                (old_name, bin_name + '_' + old_name) for old_name 
                in ['mean', 'min', 'max', 'count'] + ranks
            ), 
            inplace=True
        )
        compar_df = compar_df.merge(bin_summary_df, how='outer', on=annot)
        del(bin_summary_df)

    #Find an LCA lineage of each metaprotein (group of peptides with same annotation)
    ranks_df = pd.DataFrame(columns=[annot] + ranks)
    for bin_name in bin_names:
        bin_rank_cols = [bin_name + '_' + rank for rank in ranks]
        bin_ranks_df = compar_df[[annot] + bin_rank_cols].copy()
        bin_ranks_df.rename(
            columns=dict(
                (bin_rank_col, ranks[i]) for i, bin_rank_col in enumerate(bin_rank_cols)
            ), 
            inplace=True
        )
        ranks_df = pd.concat([ranks_df, bin_ranks_df], ignore_index=True)
        compar_df.drop(bin_rank_cols, axis=1, inplace=True)
    compar_df.sort_values(annot, inplace=True)
    compar_df.reset_index(drop=True, inplace=True)
    #Find the LCA lineage for the newly added and previously consolidated datasets
    compar_df[ranks] = merge_ranks(ranks_df.groupby(annot, as_index=False))

    # Count the number of peptides in each bin that were found for each metaprotein
    compar_df['total_count'] = 0
    for bin_name in bin_names:
        bin_count_hdr = bin_name + '_count'
        compar_df[bin_count_hdr].fillna(0, inplace=True)
        compar_df['total_count'] += compar_df[bin_count_hdr]
        compar_df[bin_count_hdr].replace(0, np.nan, inplace=True)

    compar_df.sort_values(['total_count'], ascending=False, inplace=True)
    if state == None:
        compar_table_basename = 'compar_table.' + annot + '.tsv'
    else:
        compar_table_basename = 'compar_table.' + state + '.' + annot + '.tsv'
    compar_table_fp = os.path.join(out_dir, compar_table_basename)
    compar_df.to_csv(compar_table_fp, sep='\t', index=False)

    #Make score table from comparison table
    all_bin_df = pd.DataFrame(columns=[annot])
    for bin_name in bin_names:
        bin_scores = []
        bin_df = compar_df[[bin_name + '_mean', bin_name + '_count']]
        bin_df.fillna(0, inplace=True)
        #Scale to min bitscore for significant alignment
        bin_df[bin_name + '_mean'] = bin_df[bin_name + '_mean'] - sig_cutoff
        all_bin_df[bin_name] = bin_df[bin_name + '_mean'] * bin_df[bin_name + '_count']
    all_bin_df.sort_values('annot', inplace=True)
    all_bin_df.to_csv(
        os.path.join(out_dir, annot + '_score.' + state + '.tsv'), sep='\t', index=False
    )
    #Scale metaprotein scores to 1 across bins
    all_bin_df = all_bin_df.groupby(annot).sum()
    all_bin_df = all_bin_df.div(all_bin_df.max(0), axis=1)
    #Write another table of SCALED scores
    all_bin_df.to_csv(
        os.path.join(out_dir, annot + '_scaled_score.' + state + '.tsv'), sep='\t'
    )

    return tuple([compar_table_fp])

def merge_ranks(annot_gb):
    '''
    Determine consensus (LCA) taxonomic ranks for peptides with same functional annotation
    '''

    agg_taxa_dict = OrderedDict([(rank, []) for rank in ranks])
    for _, group_df in annot_gb:
        tax_consistency = False
        for rank in ranks:
            if tax_consistency:
                agg_taxa_dict[rank].append(group_df[rank].iloc[0])
            else:
                rank_series = group_df[rank]
                if len(rank_series.unique()) == 1:
                    taxon = rank_series.iloc[0]
                    if pd.isnull(taxon):
                        agg_taxa_dict[rank].append('')
                    else:
                        taxon_consistency = True
                        agg_taxa_dict[rank].append(taxon)
                else:
                    agg_taxa_dict[rank].append('')
    ranks_df = pd.DataFrame.from_dict(agg_taxa_dict)

    return ranks_df

def compare_bins_ec(bin_table_fps, state=None, path_map=False):
    '''
    Compare bins using EC IDs and optionally Pathway Maps
    '''

    if path_map:
        ko_ec_dict, ec_map_dict, map_name_dict = load_kegg_info(path_map)
    else:
        ko_ec_dict = load_kegg_info(path_map)

    ec_compar_df = pd.DataFrame(columns=['ec'])
    map_compar_df = pd.DataFrame(columns=['map'])
    for i, bin_table_fp in enumerate(bin_table_fps):
        bin_name = bin_names[i]
        bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)[
            ['kegg', 'bitscore'] + ranks
        ]
        #Drop peptides lacking KO ID
        bin_df['kegg'] = bin_df[pd.notnull(bin_df['kegg'])]

        #Replicate rows for each KO ID with multiple values, splitting the KO ID list
        new_rows = []
        for t in bin_df.itertuples():
            for a in t[1].split(','):
                new_rows.append(tuple([a]) + t[2:])
        bin_df = pd.DataFrame(new_rows, columns=bin_df.columns)
        
        ecs = []
        for ko in bin_df['kegg'].tolist():
            try:
                ecs.append(','.join(ko_ec_dict[ko]))
            #The KO ID may not map to an EC ID
            except KeyError:
                ecs.append(np.nan)
        bin_df.rename(dict([('kegg', 'ec')]), inplace=True)
        bin_df['ec'] = ecs
        bin_df = bin_df[pd.notnull(bin_df['ec'])]
        #KO IDs can map to multiple EC IDs, so replicate rows as necessary
        new_rows = []
        for t in bin_df.itertuples():
            for a in t[1].split(','):
                new_rows.append(tuple([a]) + t[2:])
        bin_df = pd.DataFrame(new_rows, columns=bin_df.columns)

        #Make a table of count statistics for all peptides sharing each EC ID
        ec_gb = bin_df.groupby(ec)
        ec_df = ec_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
        ec_df['ec'] = [a for a, _ in ec_gb]
        ec_df.reset_index(inplace=True, drop=True)
        #Find consensus (LCA) taxonomic ranks for peptides sharing EC ID
        ec_df[ranks] = merge_ranks(ec_gb)

        #Summarize information from each bin in a table containing a block of columns for each bin
        #Sort columns
        ec_bin_summary_df = ec_df[['ec', 'mean', 'min', 'max', 'count'] + ranks]
        ec_bin_summary_df['mean'] = ec_bin_summary_df['mean'].round(1)
        ec_bin_summary_df.rename(
            columns=dict(
                (old_name, bin_name + '_' + old_name) for old_name 
                in ['mean', 'min', 'max', 'count'] + ranks
            ), 
            inplace=True
        )
        ec_compar_df = ec_compar_df.merge(ec_bin_summary_df, how='outer', on='ec')
        del(ec_bin_summary_df)

        if path_map == True:
            maps = []
            for ec in bin_df['ec'].tolist():
                try:
                    maps.append(','.join(ec_map_dict[ec]))
                #The EC ID may not map to a Pathway Map
                except KeyError:
                    maps.append(np.nan)
            bin_df.rename(dict([('ec', 'map')]), inplace=True)
            bin_df['map'] = maps
            bin_df = bin_df[pd.notnull(bin_df['map'])]
            #EC IDs can map to multiple Pathway Maps, so replicate rows as necessary
            new_rows = []
            for t in bin_df.itertuples():
                for a in t[1].split(','):
                    new_rows.append(tuple([a]) + t[2:])
            bin_df = pd.DataFrame(new_rows, columns=bin_df.columns)

            #Make a table of count statistics for all peptides sharing each Map
            map_gb = bin_df.groupby(ec)
            map_df = ec_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
            map_df['map'] = [a for a, _ in map_gb]
            map_df.reset_index(inplace=True, drop=True)
            #Find consensus (LCA) taxonomic ranks for peptides sharing Map
            map_df[ranks] = merge_ranks(map_gb)

            #Summarize information from each bin in a table containing a block of columns for each bin
            #Sort columns
            map_bin_summary_df = ec_df[['ec', 'mean', 'min', 'max', 'count'] + ranks]
            map_bin_summary_df['mean'] = map_bin_summary_df['mean'].round(1)
            map_bin_summary_df.rename(
                columns=dict(
                    (old_name, bin_name + '_' + old_name) for old_name 
                    in ['mean', 'min', 'max', 'count'] + ranks
                ), 
                inplace=True
            )
            map_compar_df = map_compar_df.merge(map_bin_summary_df, how='outer', on='ec')
            del(map_bin_summary_df)

    #Find an LCA lineage of each metaprotein (group of peptides with same annotation)
    ec_ranks_df = pd.DataFrame(columns=['ec'] + ranks)
    map_ranks_df = pd.DataFrame(columns=['map'] + ranks)
    for bin_name in bin_names:
        bin_rank_cols = [bin_name + '_' + rank for rank in ranks]
        bin_ranks_df = ec_compar_df[['ec'] + bin_rank_cols].copy()
        bin_ranks_df.rename(
            columns=dict(
                (bin_rank_col, ranks[i]) for i, bin_rank_col in enumerate(bin_rank_cols)
            ), 
            inplace=True
        )
        ec_ranks_df = pd.concat([ec_ranks_df, bin_ranks_df], ignore_index=True)
        ec_compar_df.drop(bin_rank_cols, axis=1, inplace=True)
        
        if path_map:
            bin_rank_cols = [bin_name + '_' + rank for rank in ranks]
            bin_ranks_df = map_compar_df[['map'] + bin_rank_cols].copy()
            bin_ranks_df.rename(
                columns=dict(
                    (bin_rank_col, ranks[i]) for i, bin_rank_col in enumerate(bin_rank_cols)
                ), 
                inplace=True
            )
            map_ranks_df = pd.concat([map_ranks_df, bin_ranks_df], ignore_index=True)
            map_compar_df.drop(bin_rank_cols, axis=1, inplace=True)

    ec_compar_df.sort_values('ec', inplace=True)
    ec_compar_df.reset_index(drop=True, inplace=True)
    #Find the LCA lineage for the metaproteins in the consolidated datasets
    ec_compar_df[ranks] = merge_ranks(ec_ranks_df.groupby('ec', as_index=False))

    if path_map:
        map_compar_df.sort_values('map', inplace=True)
        map_compar_df.reset_index(drop=True, inplace=True)
        #Find the LCA lineage for the metaproteins in the consolidated datasets
        map_compar_df[ranks] = merge_ranks(map_ranks_df.groupby('map', as_index=False))

    # Count the number of peptides in each bin that were found for each metaprotein
    ec_compar_df['total_count'] = 0
    for bin_name in bin_names:
        bin_count_hdr = bin_name + '_count'
        ec_compar_df[bin_count_hdr].fillna(0, inplace=True)
        ec_compar_df['total_count'] += ec_compar_df[bin_count_hdr]
        ec_compar_df[bin_count_hdr].replace(0, np.nan, inplace=True)

    compar_table_fps = []

    ec_compar_df.sort_values(['total_count'], ascending=False, inplace=True)
    if state == None:
        ec_compar_table_basename = 'compar_table.ec.tsv'
    else:
        ec_compar_table_basename = 'compar_table.' + state + '.ec.tsv'
    ec_compar_table_fp = os.path.join(out_dir, ec_compar_table_basename)
    ec_compar_df.to_csv(ec_compar_table_fp, sep='\t', index=False)
    compar_table_fps.append(ec_compar_table_fp)

    if path_map:
        # Count the number of peptides in each bin that were found for each metaprotein
        map_compar_df['total_count'] = 0
        for bin_name in bin_names:
            bin_count_hdr = bin_name + '_count'
            map_compar_df[bin_count_hdr].fillna(0, inplace=True)
            map_compar_df['total_count'] += map_compar_df[bin_count_hdr]
            map_compar_df[bin_count_hdr].replace(0, np.nan, inplace=True)

        map_compar_df.sort_values(['total_count'], ascending=False, inplace=True)
        if state == None:
            map_compar_table_basename = 'compar_table.map.tsv'
        else:
            map_compar_table_basename = 'compar_table.' + state + '.map.tsv'
        map_compar_table_fp = os.path.join(out_dir, map_compar_table_basename)
        map_compar_df.to_csv(map_compar_table_fp, sep='\t', index=False)
        compar_table_fps.append(map_compar_table_fp)

    #Make score table from comparison table
    ec_all_bin_df = pd.DataFrame(columns=['ec'])
    map_all_bin_df = pd.DataFrame(columns=['map'])
    for bin_name in bin_names:
        bin_scores = []
        ec_bin_df = ec_compar_df[[bin_name + '_mean', bin_name + '_count']]
        map_bin_df = map_compar_df[[bin_name + '_mean', bin_name + '_count']]
        ec_bin_df.fillna(0, inplace=True)
        map_bin_df.fillna(0, inplace=True)
        #Scale to min bitscore for significant alignment
        ec_bin_df[bin_name + '_mean'] = ec_bin_df[bin_name + '_mean'] - sig_cutoff
        map_bin_df[bin_name + '_mean'] = map_bin_df[bin_name + '_mean'] - sig_cutoff
        ec_all_bin_df[bin_name] = ec_bin_df[bin_name + '_mean'] * ec_bin_df[bin_name + '_count']
        map_all_bin_df[bin_name] = map_bin_df[bin_name + '_mean'] * map_bin_df[bin_name + '_count']
    ec_all_bin_df.sort_values('ec', inplace=True)
    map_all_bin_df.sort_values('map', inplace=True)
    ec_all_bin_df.to_csv(
        os.path.join(out_dir, 'ec_score.' + state + '.tsv'), sep='\t', index=False
    )
    map_all_bin_df.to_csv(
        os.path.join(out_dir, 'map_score.' + state + '.tsv'), sep='\t', index=False
    )
    #Scale metaprotein scores to 1 across bins
    ec_all_bin_df = ec_all_bin_df.groupby('ec').sum()
    ec_all_bin_df = ec_all_bin_df.div(ec_all_bin_df.max(0), axis=1)
    map_all_bin_df = map_all_bin_df.groupby('map').sum()
    map_all_bin_df = map_all_bin_df.div(map_all_bin_df.max(0), axis=1)
    #Write another table of SCALED scores
    ec_all_bin_df.to_csv(
        os.path.join(out_dir, 'ec_scaled_score.' + state + '.tsv'), sep='\t'
    )
    map_all_bin_df.to_csv(
        os.path.join(out_dir, 'map_scaled_score.' + state + '.tsv'), sep='\t'
    )

    return tuple(compar_table_fps)

def load_kegg_info(path_map):

    #Mapping of KO IDs to lists of EC IDs
    ko_ec_dict = OrderedDict()
    with open(os.path.join(kegg_dir, 'ko_ec.tsv')) as handle:
        for entry in [s.rstrip().split('\t') for s in handle.readlines()[1:]]:
            ko_ec_dict[entry[0]] = entry[1].split(',')
    if path_map:
        #Mapping of EC IDs to Map IDs
        ec_map_dict = OrderedDict()
        with open(os.path.join(kegg_dir, 'ec_map.tsv')) as handle:
            for entry in [s.rstrip().split('\t') for s in handle.readlines()[1:]]:
                ec_map_dict[entry[0]] = entry[1].split(',')
        #Mapping of Map IDs to Pathway Names
        map_name_dict = OrderedDict()
        with open(os.path.join(kegg_dir, 'map_name.tsv')) as handle:
            for entry in [s.rstrip().split('\t') for s in handle.readlines()[1:]]:
                map_name_dict[entry[0]] = entry[1]
        return ko_ec_dict, ec_map_dict, map_name_dict
    else:
        return ko_ec_dict

def compare_bins_ec_genefam(bin_table_fps, state=None):
    '''
    Compare bins using a primary and secondary functional annotation
    (for the case in which the primary annotation was not assigned)
    Currently EC ID is the primary and Gene Family the secondary annotation
    [This function can also work with GO terms with slight modification
    '''

    ko_ec_dict = load_kegg_info(path_map=False)

    compar_df = pd.DataFrame(columns=['id'])
    for i, bin_table_fp in enumerate(bin_table_fps):
        bin_name = bin_names[i]
        bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)[
            ['kegg', 'genefam', 'bitscore'] + ranks
        ]
        kegg_df = bin_df[pd.notnull(bin_df['kegg'])]
        #Secondary annotation only considered for peptides without primary annotations
        genefam_df = bin[pd.notnull(bin_df['genefam']) & pd.isnull(bin_df['kegg'])]
        
        #Replicate rows for each annotation containing multiple values
        new_lines = []
        for t in kegg_df.itertuples():
            for a in t[1].split(','):
                new_lines.append(tuple([a]) + t[3:])
        ko_df = pd.DataFrame(new_lines, columns=['ko', 'bitscore'] + ranks)

        ecs = []
        for ko in ko_df['ko'].tolist():
            try:
                ecs.append(','.join(ko_ec_dict[ko]))
            #The KO ID may not map to an EC ID
            except KeyError:
                ecs.append(np.nan)
        ko_df.rename(dict([('ko', 'ec')]), inplace=True)
        ec_df = ko_df
        ec_df['ec'] = ecs
        ec_df = ec_df[pd.notnull(ec_df['ec'])]
        #KO IDs can map to multiple EC IDs, so replicate rows as necessary
        new_rows = []
        for t in ec_df.itertuples():
            for a in t[1].split(','):
                new_rows.append(tuple([a]) + t[2:])
        ec_df = pd.DataFrame(new_rows, columns=ec_df.columns)        
        ec_df.rename(dict([('ec', 'id')]), inplace=True)

        new_lines = []
        for t in genefam_df.itertuples():
            for a in t[2].split(','):
                new_lines.append(tuple([a]) + t[3:])
        genefam_df = pd.DataFrame(new_lines, columns=['genefam', 'bitscore'] + ranks)
        genefam_df.rename(dict([('genefam', 'id')]), inplace=True)

        id_df = pd.concat([ec_df, genefam_df], ignore_index=True)
        id_gb = id_df.groupby('id')
        id_df = id_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
        id_df['id'] = [a for a, _ in id_gb]
        id_df.reset_index(inplace=True, drop=True)
        id_df[ranks] = merge_ranks(id_gb)

        #Summarize the information from each bin in a table, 
        #with a block of columns for each bin
        bin_summary_df = id_df[
            ['id', 'mean', 'min', 'max', 'count'] + ranks
        ]
        bin_summary_df['mean'] = bin_summary_df['mean'].round(1)
        bin_summary_df.rename(
            columns=dict(
                (old_name, bin_name + '_' + old_name) for old_name 
                in ['mean', 'min', 'max', 'count'] + ranks
            ), 
            inplace=True
        )
        compar_df = compar_df.merge(bin_summary_df, how='outer', on='id')
        del(bin_summary_df)

    #Find an LCA lineage of each metaprotein (group of peptides with same annotation)
    id_ranks_df = pd.DataFrame(columns=['id'] + ranks)
    for bin_name in bin_names:
        bin_rank_cols = [bin_name + '_' + rank for rank in ranks]
        bin_ranks_df = compar_df[['id'] + bin_rank_cols].copy()
        bin_ranks_df.rename(
            columns=dict(
                (bin_rank_col, ranks[i]) for i, bin_rank_col in enumerate(bin_rank_cols)
            ), 
            inplace=True
        )
        id_ranks_df = pd.concat([id_ranks_df, bin_ranks_df], ignore_index=True)
        id_compar_df.drop(bin_rank_cols, axis=1, inplace=True)

    id_compar_df.sort_values('id', inplace=True)
    id_compar_df.reset_index(drop=True, inplace=True)
    #Find the LCA lineage for the metaproteins in the consolidated datasets
    id_compar_df[ranks] = merge_ranks(id_ranks_df.groupby('id', as_index=False))

    #Count the number of peptides in each bin that were found for each metaprotein
    id_compar_df['total_count'] = 0
    for bin_name in bin_names:
        bin_count_hdr = bin_name + '_count'
        id_compar_df[bin_count_hdr].fillna(0, inplace=True)
        id_compar_df['total_count'] += id_compar_df[bin_count_hdr]
        id_compar_df[bin_count_hdr].replace(0, np.nan, inplace=True)

    id_compar_df.sort_values(['total_count'], ascending=False, inplace=True)
    if state == None:
        id_compar_table_basename = 'compar_table.ec+genefam.tsv'
    else:
        id_compar_table_basename = 'compar_table.' + state + '.ec+genefam.tsv'
    id_compar_table_fp = os.path.join(out_dir, id_compar_table_basename)
    id_compar_df.to_csv(id_compar_table_fp, sep='\t', index=False)

    #Make score table from comparison table
    all_bin_df = pd.DataFrame(columns=['id'])
    for bin_name in bin_names:
        bin_scores = []
        bin_df = id_compar_df[[bin_name + '_mean', bin_name + '_count']]
        bin_df.fillna(0, inplace=True)
        #Scale to min bitscore for significant alignment
        bin_df[bin_name + '_mean'] = bin_df[bin_name + '_mean'] - sig_cutoff
        all_bin_df[bin_name] = bin_df[bin_name + '_mean'] * bin_df[bin_name + '_count']
    all_bin_df.sort_values('id', inplace=True)
    all_bin_df.to_csv(
        os.path.join(out_dir, 'ec_genefam_score.' + state + '.tsv'), sep='\t', index=False
    )
    #Scale metaprotein scores to 1 across bins
    all_bin_df = all_bin_df.groupby('id').sum()
    all_bin_df = all_bin_df.div(all_bin_df.max(0), axis=1)
    #Write another table of SCALED scores
    all_bin_df.to_csv(
        os.path.join(out_dir, 'id_scaled_score.' + state + '.tsv'), sep='\t'
    )

    return tuple([id_compar_table_fp])

def compare_bins_genefam_descrip(sys_bin_table_fps, man_group_dict, state=None):
    '''
    Compare bins using manually curated functional categories
    comprised of Gene Families and eggNOG Descriptions.
    Each Gene Family (or Description for peptides without Gene Family) maps to a unique COG.
    '''

    genefam_descrip_compar_df = pd.DataFrame(columns=['genefam', 'descrip', 'cog', 'group'])
    man_group_compar_df = pd.DataFrame(columns=['group', 'genefam', 'descrip', 'cog'])
    for bin_table_sys_descrip_fp, bin_name in zip(sys_bin_table_fps, bin_names):
        
        bin_df = pd.read_csv(bin_table_sys_descrip_fp, sep='\t', header=0)[
            ['genefam', 'descrip', 'cog', 'bitscore'] + ranks
        ]
        #First, consider peptides with Gene Family annotations
        genefam_gb = bin_df[pd.notnull(bin_df['genefam'])].groupby('genefam')
        genefam_df = genefam_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
        genefam_df['genefam'] = [genefam for genefam, _ in genefam_gb]
        genefam_df['descrip'] = genefam_gb['descrip'].agg(lambda d: d.value_counts().index[0])
        genefam_df['cog'] = genefam_gb['cog'].agg(lambda c: c.value_counts().index[0])
        #Assign Gene Families to manual groups
        genefam_group_dict = man_group_dict['genefam']
        man_groups = []
        for genefam in genefam_df['genefam'].tolist():
            try:
                man_groups.append(genefam_group_dict[genefam])
            except KeyError:
                #Gene Family was not assigned to a group
                man_groups.append('')
        genefam_df['group'] = man_groups
        genefam_df.reset_index(inplace=True, drop=True)
        genefam_df[ranks] = merge_ranks(genefam_gb)
        del(genefam_gb)
        #Second, consider peptides that only have eggNOG Descriptions
        bin_df['descrip_lowercase'] = bin_df['descrip'].str.lower()
        descrip_gb = bin_df[pd.isnull(bin_df['genefam'])].groupby('descrip_lowercase')
        descrip_df = descrip_gb['bitscore'].agg(['mean', 'min', 'max', 'count'])
        descrip_df['descrip'] = descrip_gb['descrip'].agg(lambda d: d.value_counts().index[0])
        descrip_df['cog'] = descrip_gb['cog'].agg(lambda c: c.value_counts().index[0])
        #Assign Decsriptions to manual groups
        descrip_group_dict = man_group_dict['descrip']
        man_groups = []
        for descrip in descrip_df['descrip'].tolist():
            try:
                man_groups.append(descrip_group_dict[descrip])
            except KeyError:
                #Gene Family was not assigned to a group
                man_groups.append('')
        descrip_df['group'] = man_groups
        descrip_df.reset_index(inplace=True, drop=True)
        descrip_df[ranks] = merge_ranks(descrip_gb)
        del(descrip_gb)
        del(bin_df)
        #Summarize the information from each bin in a table, with a block of columns for each bin
        bin_summary_df = pd.concat([genefam_df, descrip_df], ignore_index=True)
        del(genefam_df)
        del(descrip_df)
        bin_summary_df = bin_summary_df[
            ['genefam', 'descrip', 'cog', 'group', 'mean', 'min', 'max', 'count'] + ranks
        ]
        bin_summary_df['mean'] = bin_summary_df['mean'].round(1)
        bin_summary_df.rename(
            columns=dict(
                (old_name, bin_name + '_' + old_name) for old_name 
                in ['mean', 'min', 'max', 'count'] + ranks
            ), 
            inplace=True
        )
        genefam_descrip_compar_df = genefam_descrip_compar_df.merge(
            bin_summary_df, 
            how='outer', 
            on=['genefam', 'descrip', 'cog', 'group']
        )
        #Make a bin comparison table that only considers metaproteins with manual group annotation
        man_group_compar_df = man_group_compar_df.merge(
            bin_summary_df[bin_summary_df['group'] != ''], 
            how='outer', 
            on=['group', 'genefam', 'descrip', 'cog']
        )
        del(bin_summary_df)

    #Find an LCA lineage of each "metaprotein"
    ranks_df = pd.DataFrame(columns=['genefam', 'descrip', 'cog', 'group'] + ranks)
    for bin_name in bin_names:
        bin_rank_cols = [bin_name + '_' + rank for rank in ranks]
        bin_ranks_df = genefam_descrip_compar_df[
            ['genefam', 'descrip', 'cog', 'group'] + bin_rank_cols
        ].copy()
        bin_ranks_df.rename(
            columns=dict(
                (bin_rank_col, ranks[i]) for i, bin_rank_col in enumerate(bin_rank_cols)
            ), 
            inplace=True
        )
        ranks_df = pd.concat([ranks_df, bin_ranks_df], ignore_index=True)
        genefam_descrip_compar_df.drop(bin_rank_cols, axis=1, inplace=True)
    genefam_descrip_compar_df['genefam'] = genefam_descrip_compar_df['genefam'].fillna('')
    genefam_descrip_compar_df.sort_values(['genefam', 'descrip', 'cog', 'group'], inplace=True)
    genefam_descrip_compar_df.reset_index(drop=True, inplace=True)
    ranks_df['genefam'] = ranks_df['genefam'].fillna('')
    ranks_df['group'] = ranks_df['group'].fillna('')
    merged_ranks_df = merge_ranks(
        ranks_df.groupby(['genefam', 'descrip', 'cog', 'group'], as_index=False)
    )
    genefam_descrip_compar_df[ranks] = merged_ranks_df
    man_group_compar_df[ranks] = merged_ranks_df

    #Count the number of peptides in each bin that were found for the protein
    genefam_descrip_compar_df['total_count'] = 0
    for bin_name in bin_names:
        bin_count_hdr = bin_name + '_count'
        genefam_descrip_compar_df[bin_count_hdr].fillna(0, inplace=True)
        genefam_descrip_compar_df['total_count'] += genefam_descrip_compar_df[bin_count_hdr]
        genefam_descrip_compar_df[bin_count_hdr].replace(0, np.nan, inplace=True)
    man_group_compar_df = pd.merge(
        man_group_compar_df, 
        genefam_descrip_compar_df[
            genefam_descrip_compar_df['group'] != ''
        ][['group', 'total_count']], 
        how='inner', 
        on='group'
    )

    genefam_descrip_compar_df.sort_values(
        ['cog', 'total_count', 'group'], ascending=[True, False, True], inplace=True
    )
    man_group_compar_df.sort_values('group', inplace=True)
    if state == None:
        genefam_descrip_compar_table_basename = 'compar_table.genefam+descrip.tsv'
        manual_group_compar_table_basename = 'compar_table.manual_group.tsv'
    else:
        genefam_descrip_compar_table_basename = 'compar_table.' + state + '.genefam+descrip.tsv'
        manual_group_compar_table_basename = 'compar_table.' + state + '.manual_group.tsv'
    genefam_descrip_compar_table_fp = os.path.join(out_dir, genefam_descrip_compar_table_basename)
    manual_group_compar_table_fp = os.path.join(out_dir, manual_group_compar_table_basename)
    genefam_descrip_compar_df.to_csv(genefam_descrip_compar_table_fp, sep='\t', index=False)
    manual_group_compar_df.to_csv(manual_group_compar_table_fp, sep='\t', index=False)




    #Make score table from comparison table
    genefam_descrip_all_bin_df = pd.DataFrame(columns=['genefam', 'descrip'])
    group_all_bin_df = pd.DataFrame(columns=['group'])
    for bin_name in bin_names:
        bin_scores = []
        genefam_descrip_bin_df = genefam_descrip_compar_df[
            [bin_name + '_mean', bin_name + '_count']
        ]
        group_bin_df = manual_group_compar_df[[bin_name + '_mean', bin_name + '_count']]
        genefam_descrip_bin_df.fillna(0, inplace=True)
        group_bin_df.fillna(0, inplace=True)
        #Scale to min bitscore for significant alignment
        genefam_descrip_bin_df[bin_name + '_mean'] = \
            genefam_descrip_bin_df[bin_name + '_mean'] - sig_cutoff
        group_bin_df[bin_name + '_mean'] = group_bin_df[bin_name + '_mean'] - sig_cutoff
        genefam_descrip_all_bin_df[bin_name] = \
            genefam_descrip_bin_df[bin_name + '_mean'] * \
            genefam_descrip_bin_df[bin_name + '_count']
        group_all_bin_df[bin_name] = \
            group_bin_df[bin_name + '_mean'] * \
            group_bin_df[bin_name + '_count']
    genefam_descrip_all_bin_df.sort_values(['genefam', 'descrip'], inplace=True)
    group_all_bin_df.sort_values('group', inplace=True)
    genefam_descrip_all_bin_df.to_csv(
        os.path.join(out_dir, 'genefam_decrip_score.' + state + '.tsv'), sep='\t', index=False
    )
    group_all_bin_df.to_csv(
        os.path.join(out_dir, 'manual_group_score.' + state + '.tsv'), sep='\t', index=False
    )
    #Scale metaprotein scores to 1 across bins
    genefam_descrip_all_bin_df = genefam_descrip_all_bin_df.groupby(['genefam', 'descrip']).sum()
    genefam_descrip_all_bin_df = genefam_descrip_all_bin_df.div(
        genefam_descrip_all_bin_df.max(0), axis=1
    )
    group_all_bin_df = group_all_bin_df.groupby('group').sum()
    group_all_bin_df = group_all_bin_df.div(group_all_bin_df.max(0), axis=1)
    #Write another table of SCALED scores
    genefam_descrip_all_bin_df.to_csv(
        os.path.join(out_dir, 'genefam_descrip_scaled_score.' + state + '.tsv'), sep='\t'
    )
    group_all_bin_df.to_csv(
        os.path.join(out_dir, 'manual_group_scaled_score.' + state + '.tsv'), sep='\t'
    )

    return tuple([genefam_descrip_compar_table_fp, manual_group_compar_table_fp])

def systematize_bin_tables(bin_table_fps, sys_dict):
    
    genefam_descrip_dict = sys_dict['genefam_descrip']
    genefam_cog_dict = sys_dict['genefam_cog']
    descrip_cog_dict = sys_dict['descrip_cog']

    #Use the new annotations to modify those in the bin tables
    sys_bin_table_fps = []
    for bin_table_fp in bin_table_fps:
        bin_df = pd.read_csv(bin_table_fp, sep='\t', header=0)
        bin_df['genefam'] = bin_df['genefam'].fillna('')
        bin_df['descrip'] = bin_df['descrip'].fillna('')
        bin_df['cog'] = bin_df['cog'].fillna('')
        genefams = bin_df['genefam'].tolist()
        old_descrips = bin_df['descrip'].tolist()
        new_descrips = []
        new_cogs = []
        for genefam, descrip in zip(genefams, old_descrips):
            if genefam == '':
                new_descrips.append(descrip)
                new_cogs.append(descrip_cog_dict[descrip])
            else:
                new_descrips.append(genefam_descrip_dict[genefam])
                new_cogs.append(genefam_cog_dict[genefam])
        bin_df['descrip'] = new_descrips
        bin_df['cog'] = new_cogs
        sys_bin_table_f = os.path.basename(bin_table_fp).replace(
            '.blast_out.txt', 'blast_out.sys.txt'
        )
        sys_bin_table_fp = os.path.join(out_dir, sys_bin_table_f)
        sys_bin_table_fps.append(sys_bin_table_fp)
        bin_df.to_csv(sys_bin_table_fp, sep='\t', index=False)

    return sys_bin_table_fps

def compare_states(state_compar_table_fp_dict, annot_method):

    if annot_method == 'genefam':
        annot_hdrs = ['genefam']
    elif annot_method == 'ec':
        annot_hdrs = ['ec']
    elif annot_method == 'map':
        annot_hdrs = ['map']
    elif annot_method == 'ec+genefam':
        annot_hdrs = ['id']
    elif annot_method == 'genefam+descrip':
        annot_hdrs = ['genefam', 'descrip', 'cog']
    elif annot_method == 'manual_group':
        annot_hdrs = ['group']

    state_compar_table_dict = OrderedDict([
        (state, pd.read_csv(compar_table_fp, sep='\t', header=0)) 
        for state, compar_table_fp in state_compar_table_fp_dict.items()
    ])

    #Merge sample columns from each state's sample comparison table into a single table
    num_cols = len(list(state_compar_table_dict.values())[0].columns)
    all_state_df = pd.DataFrame(columns=annot_hdrs)
    #Loop through each column SUBSEQUENT to the annotation columns
    for i in range(num_cols - len(annot_hdrs)):
        #Start with the first state, with the state name being used as the merged column suffix
        prev_state_suff = '_' + list(state_compar_table_dict.keys())[0]
        col_df = list(state_compar_table_dict.values())[0]
        col = col_df.columns[i + len(annot_hdrs)]
        col_df = col_df[annot_hdrs + [col]]
        col_df.rename(columns={col: col + prev_state_suff}, inplace=True)
        #Proceed with the other states
        for state, compar_df in list(state_compar_table.items())[1:]:
            state_suff = '_' + state
            next_col_df = compar_df[annot_hdrs + [col]]
            next_col_df.rename(columns={col: col + state_suff}, inplace=True)
            col_df = col_df.merge(
                next_col_df, 
                how='outer', 
                on=annot_hdrs, 
                suffixes=(prev_state_suff, state_suff)
            )
            prev_state_suff = state_suff
        #Merge the column group into the overall table by shared annotation
        all_state_df = all_state_df.merge(
            col_df, how='outer', on=annot_hdrs
        )

    #Find the bin with the maximum average alignment for each metaprotein in a state
    #This indicates the bin with which the metaprotein is most strongly affiliated in the state
    for state in all_states:
        all_state_df[state + '_max_mean'] = all_state_df[
            [bin_name + '_mean_' + state for bin_name in bin_names]
        ].max(1)

    #Total count of PSMs for each metaprotein across states
    all_state_df['total_count'] = all_state_df[
        ['total_count_' + state for state in all_states]
    ].copy().fillna(0).sum()

    all_state_df.sort_values(['cog', 'total_count'], ascending=[True, False], inplace=True)
    all_state_table_fp = os.path.join(out_dir, 'state_compar.' + annot_method + '.tsv')
    all_state_df.to_csv(all_state_table_fp, sep='\t', index=False)

    return

def nsaf_analysis(prot_state_dict, postnovo_table_f, genefam_descrip_dict, man_group_dict):

    prot_postnovo_df_dict = OrderedDict.fromkeys(prot_names)
    #Spectrum count per sample
    prot_count_dict = OrderedDict([(prot_name, 0) for prot_name in prot_state_dict])
    #Spectrum count per state
    state_count_dict = OrderedDict([(state, 0) for state in all_states])
    for prot_name, prot_dir in zip(prot_names, prot_dirs):
        # Calculate SAF for each unique peptide in a sample
        postnovo_table_fp = os.path.join(prot_dir, postnovo_table_f)
        postnovo_df = pd.read_csv(postnovo_table_fp, sep='\t', header=0)
        postnovo_df = postnovo_df[pd.notnull(postnovo_df['protein length'])]
        postnovo_df['saf'] = postnovo_df['scan count'] / postnovo_df['protein length']
        prot_count_dict[prot_name] = postnovo_df['scan count'].sum()

    #NSAF statistics are calculated for EC IDs and Pathway Maps in each proteomic dataset
    #Tables for plotting Pathway Map results in Vanted are written
    #Gene Family can be specified as a secondary annotation for peptides without EC ID
    ec_nsaf_analysis(
        prot_state_dict, prot_postnovo_df_dict, prot_count_dict, use_genefam=True
    )
    #Gene Family
    genefam_nsaf_analysis(
        prot_state_dict, prot_postnovo_df_dict, prot_count_dict
    )
    #GO Terms
    go_nsaf_analysis(
        prot_state_dict, prot_postnovo_df_dict, prot_count_dict
    )
    #Gene Family + Description
    #Manual functional group definitions
    #A systematized Postnovo output table is used
    #Modify the unsystematized bin tables with the systematized annotations
    prot_sys_postnovo_df_dict = make_sys_postnovo_tables(
        postnovo_table_f, genefam_descrip_dict, man_group_dict
    )
    genefam_descrip_nsaf_analysis(
        prot_state_dict, prot_postnovo_df_dict, prot_count_dict, prot_sys_postnovo_df_dict
    )

    return

def ec_nsaf_analysis(
    prot_state_dict, prot_postnovo_df_dict, prot_count_dict, use_genefam=False
):
    '''
    Calculate NSAF values for EC ID-centric metaproteins
    '''

    #Mappings of KO ID to EC ID to Pathway Map
    ko_ec_dict, ec_map_dict, map_name_dict = load_kegg_info(path_map=True)

    #SAF for each EC ID metaprotein in a sample
    prot_ec_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #Adjusted SAF for each EC ID metaprotein in a sample
    #Adjustment means scaling the count to the number of EC IDs reported for a peptide
    #The occurrence of multiple EC IDs reflects uncertainty in the functional assignment
    prot_ec_adj_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    if use_genefam:
        prot_genefam_saf_dict = OrderedDict(
            [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
        )
        prot_genefam_adj_saf_dict = OrderedDict(
            [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
        )
    #NSAF for each EC ID metaprotein in a sample
    prot_ec_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #Adjusted NSAF for each EC ID metaprotein in a sample
    prot_ec_adj_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #NSAF for each Map Pathway in a sample
    prot_map_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #Adjusted NSAF for each Map Pathway in a sample
    prot_map_adj_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #If secondary Gene Family annotation is considered
    if use_genefam:
        #NSAF for each Gene Family without a corresponding KEGG annotation in a sample
        prot_genefam_nsaf_dict = OrderedDict(
            [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
        )
        #Adjusted NSAF for each Gene Family without a corresponding KEGG annotation in a sample
        prot_genefam_adj_nsaf_dict = OrderedDict(
            [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
        )
    #Mapping of Map ID to represented EC IDs
    map_ec_dict = OrderedDict()

    for prot_name, postnovo_df in prot_postnovo_df_dict.items():
        #Peptides with KO IDs
        kegg_df = postnovo_df[pd.notnull(postnovo_df['kegg pathways'])]
        if use_genefam:
            genefam_df = postnovo_df[
                pd.isnull(postnovo_df['kegg pathways']) & 
                pd.notnull(postnovo_df['protein'])
            ]
            genefam_df.rename(dict([('protein', 'genefam')]), inplace=True)

        #Record sample SAF values for peptides with KEGG KO IDs
        ec_saf_dict = prot_ec_saf_dict[prot_name]
        ec_adj_saf_dict = prot_ec_saf_dict[prot_name]
        for ko_entry, saf in zip(
            kegg_df['kegg pathways'].tolist(), kegg_df['saf'].tolist()
        ):
            kos = ko_entry.split(',')
            ko_count = len(kos)
            for ko in kos:
                try:
                    for ec in ko_ec_dict[ko]:
                        try:
                            ec_saf_dict[ec] += saf
                        except KeyError:
                            ec_saf_dict[ec] = saf

                        adj_saf = saf / ko_count
                        try:
                            ec_adj_saf_dict[ec] += adj_saf
                        except KeyError:
                            ec_adj_saf_dict[ec] = adj_saf
                #KO ID doesn't map to an EC ID
                except KeyError:
                    pass
        if use_genefam:
            #Record sample SAF values for peptides without KEGG KO IDs but with Gene Family
            genefam_saf_dict = prot_genefam_saf_dict[prot_name]
            genefam_adj_saf_dict = prot_genefam_saf_dict[prot_name]
            for genefam_entry, saf in zip(
                genefam_df['genefam'].tolist(), genefam_df['saf'].tolist()
            ):
                genefams = genefam_entry.split(',')
                genefam_count = len(genefams)
                for genefam in genefams:
                    try:
                        genefam_saf_dict[genefam] += saf
                    except KeyError:
                        genefam_saf_dict[genefam] = saf

    #Calculate NSAF by dividing each SAF by sample PSM count
    for prot_name, ec_saf_dict in prot_ec_saf_dict.items():
        ec_adj_saf_dict = prot_ec_adj_saf_dict[prot_name]
        ec_nsaf_dict = prot_ec_nsaf_dict[prot_name]
        ec_adj_nsaf_dict = prot_ec_adj_nsaf_dict[prot_name]
        if use_genefam:
            genefam_saf_dict = prot_genefam_saf_dict[prot_name]
            genefam_nsaf_dict = prot_genefam_nsaf_dict[prot_name]
            genefam_adj_saf_dict = prot_genefam_adj_saf_dict[prot_name]
            genefam_adj_nsaf_dict = prot_genefam_adj_nsaf_dict[prot_name]

        map_nsaf_dict = prot_map_nsaf_dict[prot_name]
        map_adj_nsaf_dict = prot_map_adj_nsaf_dict[prot_name]

        prot_count = prot_count_dict[prot_name]

        for ec, saf in ec_saf_dict.items():
            adj_saf = ec_adj_saf_dict[ec]

            nsaf = saf / prot_count
            adj_nsaf = adf_saf / prot_count
            ec_nsaf_dict[ec] = nsaf
            ec_adj_nsaf_dict[ec] = adj_nsaf

            #Determine the Pathway Maps associated with the EC IDs
            try:
                for map_id in ec_map_dict[ec]:
                    #Find identified EC IDs within each Pathway Map
                    try:
                        map_ec_dict[map_id].append(ec)
                    except KeyError:
                        map_ec_dict[map_id] = [ec]

                    try:
                        map_nsaf_dict[map_id] += nsaf
                    except KeyError:
                        map_nsaf_dict[map_id] = nsaf
                    try:
                        map_adj_nsaf_dict[map_id] += adj_nsaf
                    except KeyError:
                        map_adj_nsaf_dict[map_id] = adj_nsaf
            #EC ID doesn't map to a Map ID
            except KeyError:
                pass

        if use_genefam:
            for genefam, saf in genefam_saf_dict.items():
                adj_saf = genefam_adj_saf_dict[genefam]

                nsaf = saf / prot_count
                adj_nsaf = adj_saf / prot_count
                genefam_nsaf_dict[genefam] = nsaf
                genefam_adj_nsaf_dict[genefam] = adj_nsaf

    #Dereplicate the list of sample EC IDs for each Map ID
    for map_id, ecs in map_ec_dict.items():
        map_ec_dict[map_id] = list(set(ecs))

    make_vanted_tables(
        prot_ec_nsaf_dict, map_ec_dict, map_name_dict, prot_state_dict
    )
    make_vanted_tables(
        prot_ec_adj_nsaf_dict, map_ec_dict, map_name_dict, prot_state_dict, adj=True
    )

    #Write tables of NSAF values for multivariate analysis
    nsaf_data_dir = os.path.join(out_dir, 'nsaf_data')
    adj_nsaf_data_dir = os.path.join(out_dir, 'adj_nsaf_data')
    if not os.path.exists(nsaf_data_dir):
        os.mkdir(nsaf_data_dir)
    if not os.path.exists(adj_nsaf_data_dir):
        os.mkdir(adj_nsaf_data_dir)

    for i, t in enumerate(prot_state_dict.items()):
        prot_name = t[0]
        state = t[1]
        ec_nsaf_f = 'EC_NSAF.samples.tsv'
        ec_nsaf_fp = os.path.join(nsaf_data_dir, ec_nsaf_f)
        ec_adj_nsaf_f = 'EC_ADJ_NSAF.samples.tsv'
        ec_adj_nsaf_fp = os.path.join(adj_nsaf_data_dir, ec_adj_nsaf_f)
        map_nsaf_f = 'MAP_NSAF.samples.tsv'
        map_nsaf_fp = os.path.join(nsaf_data_dir, map_nsaf_f)
        map_adj_nsaf_f = 'MAP_NSAF.samples.tsv'
        map_adj_nsaf_fp = os.path.join(adj_nsaf_data_dir, map_adj_nsaf_f)

        if use_genefam:
            genefam_nsaf_f = 'EC+GENEFAM_NSAF.samples.tsv'
            genefam_nsaf_fp = os.path.join(nsaf_data_dir, genefam_nsaf_f)
            genefam_adj_nsaf_f = 'EC+GENEFAM_ADJ_NSAF.samples.tsv'
            genefam_adj_nsaf_fp = os.path.join(adj_nsaf_data_dir, genefam_adj_nsaf_f)

        ec_nsaf_df = pd.DataFrame(
            list(prot_ec_nsaf_dict[prot_name].items())
        ).set_index(0)
        #Record both state and sample of the single column of NSAF data
        ec_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']
        )
        ec_adj_nsaf_df = pd.DataFrame(
            list(prot_ec_adj_nsaf_dict[prot_name].items())
        ).set_index(0)
        ec_adj_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']
        )
        map_nsaf_df = pd.DataFrame(
            list(prot_map_nsaf_dict[prot_name].items())
        ).set_index(0)
        map_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']
        )
        map_adj_nsaf_df = pd.DataFrame(
            list(prot_map_adj_nsaf_dict[prot_name].items())
        ).set_index(0)
        map_adj_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']
        )

        if use_genefam:
            genefam_nsaf_df = pd.DataFrame(
                list(prot_genefam_nsaf_dict[prot_name].items())
            ).set_index(0)
            genefam_nsaf_df.columns = pd.MultiIndex.from_arrays(
                [[state], [prot_name]], names=['State', 'Sample']    
            )
            genefam_adj_nsaf_df = pd.DataFrame(
                list(prot_genefam_adj_nsaf_dict[prot_name].items())
            ).set_index(0)
            genefam_adj_nsaf_df.columns = pd.MultiIndex.from_arrays(
                [[state], [prot_name]], names=['State', 'Sample']    
            )

        if i == 0:
            full_ec_nsaf_df = ec_nsaf_df
            full_ec_adj_nsaf_df = ec_adj_nsaf_df
            full_map_nsaf_df = map_nsaf_df
            full_map_adj_nsaf_df = map_adj_nsaf_df

            if use_genefam:
                full_genefam_nsaf_df = genefam_nsaf_df
                full_genefam_adj_nsaf_df = genefam_adj_nsaf_df

        else:
            full_ec_nsaf_df = pd.merge(
                full_ec_nsaf_df, 
                ec_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )
            full_ec_adj_nsaf_df = pd.merge(
                full_ec_adj_nsaf_df, 
                ec_adj_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )
            full_map_nsaf_df = pd.merge(
                full_map_nsaf_df, 
                map_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )
            full_map_adj_nsaf_df = pd.merge(
                full_map_adj_nsaf_df, 
                map_adj_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )

            if use_genefam:
                full_genefam_nsaf_df = pd.merge(
                    full_genefam_nsaf_df, 
                    genefam_nsaf_df, 
                    how='outer', 
                    left_index=True, 
                    right_index=True
                )
                full_genefam_adj_nsaf_df = pd.merge(
                    full_genefam_adj_nsaf_df, 
                    genefam_adj_nsaf_df, 
                    how='outer', 
                    left_index=True, 
                    right_index=True
                )

    #Sample/state headers and EC ID indices are written along with data
    full_ec_nsaf_df.to_csv(ec_nsaf_fp, sep='\t')
    full_ec_adj_nsaf_df.to_csv(ec_adj_nsaf_fp, sep='\t')
    full_map_nsaf_df.to_csv(map_nsaf_fp, sep='\t')
    full_map_adj_nsaf_df.to_csv(map_adj_nsaf_fp, sep='\t')
    if use_genefam:
        full_ec_genefam_nsaf_df = pd.concat([full_ec_nsaf_df, full_genefam_nsaf_df])
        full_ec_genefam_adj_nsaf_df = pd.concat([full_ec_adj_nsaf_df, full_genefam_adj_nsaf_df])
        full_ec_genefam_nsaf_df.to_csv(genefam_nsaf_fp, sep='\t')
        full_ec_genefam_adj_nsaf_df.to_csv(genefam_adj_nsaf_fp, sep='\t')

    return

def make_vanted_tables(prot_ec_nsaf_dict, map_ec_dict, map_name_dict, prot_state_dict, adj=False):
    ''''
    Make a Vanted-formatted table for each Map ID, filling in with info for each identified EC ID
    '''

    #Set up a Pathway Map table for plotting in Vanted
    vanted_cols, vanted_row_name_dict = vanted_col_setup() 

    #Add general Vanted input table information for each sample
    for i, t in enumerate(state_prots_dict.items()):
        state = t[0]
        prot_names = t[1]
        #Vanted input tables are inflexible in their labeling
        vanted_cols[i + 1][vanted_row_names['Plants/Genotypes**']] = i + 1
        vanted_cols[i + 1][vanted_row_names['Species']] = state
        vanted_cols[i + 1][vanted_row_names['Genotype']] = 'Mixed'
        for j, prot_name in enumerate(prot_names):
            #Count each state as a "Plant/Genotype"
            vanted_cols[0].append(i + 1)
            #Count each sample as a "Replicate"
            vanted_cols[1].append(j + 1)
            #No time series
            vanted_cols[2].append(0)
            vanted_cols[3].append('day')
    
    map_id_table_dict = OrderedDict()
    #Sum NSAF for all EC IDs in Map
    map_id_nsaf_dict = OrderedDict()
    #Loop through each unique Map found in the proteomic datasets
    for map_id in sorted([map_id for map_id in map_ec_dict.keys()]):
        map_id_table_dict[map_id] = map_vanted_cols = deepcopy(vanted_cols)
        map_id_nsaf_dict[map_id] = 0
        #Loop through each identified EC ID in the Map
        for i, ec in enumerate(map_ec_dict[map_id]):
            #Add extra columns to the table to accommodate more EC IDs
            if 5 + i >= len(map_vanted_cols):
                map_vanted_cols[5 + i] = [''] * 22
            map_vanted_cols[5 + i][19] = ec
            map_vanted_cols[5 + i][20] = 'MS'
            map_vanted_cols[5 + i][21] = 'NSAF Score'
            #Add metaprotein NSAF data for each dataset
            for state, prot_names in state_prots_dict.items():
                for prot_name in prot_names:
                    #Determine the NSAF for the metaprotein in the dataset
                    try:
                        nsaf = prot_ec_nsaf_dict[prot_name][ec]
                    except KeyError:
                        nsaf = 0
                    map_id_nsaf_dict[map_id] += nsaf
                    map_vanted_cols[5 + i].append(str(nsaf))

    #Write the Vanted-formatted tables to Excel spreadsheets
    if adj:
        kegg_map_dir = os.path.join(out_dir, 'vanted_kegg_map_adj_nsaf_tables')
    else:
        kegg_map_dir = os.path.join(out_dir, 'vanted_kegg_map_nsaf_tables')
    if not os.path.exists(kegg_map_dir):
        os.mkdir(kegg_map_dir)
    #Sum NSAF values for each map to determine which contain the most data
    total_map_nsaf_data = []
    for map_id, vanted_map_cols in map_id_table_dict.items():
        #Equalize column lengths with empty cells
        max_len = max([len(col) for col in vanted_map_cols.values()])
        for i, col in vanted_map_cols.items():
            while len(col) < max_len:
                col.append('')
        vanted_df = pd.DataFrame(vanted_map_cols)
        map_name = map_name_dict[map_id]
        #Replace backslashes in Map Names, e.g., "Glycolysis / Gluconeogenesis"
        vanted_f = map_name.replace('/', '-') + '.xlsx'
        vanted_fp = os.path.join(kegg_map_dir, vanted_f)
        writer = pd.ExcelWriter(vanted_fp, engine='xlsxwriter')
        vanted_df.to_excel(writer, sheet_name='VANTED Input Form', header=False, index=False)
        date_format = writer.book.add_format()
        date_format.set_num_format('dd/mm/yyyy')
        writer.sheets['VANTED Input Form'].write(3, 1, '1/1/2018', date_format)
        writer.save()
        total_map_nsaf_data.append([map_id, map_name, map_id_nsaf_dict[map_id]])
    total_map_nsaf_df = pd.DataFrame(
        total_map_nsaf_data, columns=['Pathway Map', 'Pathway Map Name', 'Total Score']
    )
    if adj:
        total_map_nsaf_fp = os.path.join(kegg_map_dir, 'total_map_nsaf.tsv')
    else:
        total_map_nsaf_fp = os.path.join(kegg_map_dir, 'total_map_adj_nsaf.tsv')
    total_map_nsaf_df.to_csv(total_map_nsaf_fp, sep='\t', index=False)

    return

def vanted_col_setup():
    '''
    Set up a table formatted for plotting Pathway Map graphs in Vanted
    '''

    vanted_cols = []
    #Row 0
    vanted_cols.append([''] * 22)
    vanted_cols[-1][0] = 'VANTED - Input File'
    vanted_cols[-1][2] = 'Experiment'
    vanted_cols[-1][3] = 'Start of Experiment (Date)'
    vanted_cols[-1][4] = 'Remark*'
    vanted_cols[-1][5] = 'Experiment Name (ID)'
    vanted_cols[-1][6] = 'Coordinator'
    vanted_cols[-1][7] = 'Sequence-Name*'
    vanted_cols[-1][10] = 'Plants/Genotypes**'
    vanted_cols[-1][11] = 'Species'
    vanted_cols[-1][12] = 'Variety*'
    vanted_cols[-1][13] = 'Genotype'
    vanted_cols[-1][14] = 'Growth conditions*'
    vanted_cols[-1][15] = 'Treatment*'
    vanted_cols[-1][19] = 'Measurements'
    vanted_cols[-1][21] = 'Plant/Genotype***'

    #Row 1
    vanted_cols.append([''] * 22)
    vanted_cols[-1][3] = '1/1/2018'
    vanted_cols[-1][5] = 'Experiment'
    vanted_cols[-1][6] = 'Scientist'
    vanted_cols[-1][21] = 'Replicate #'

    #Row 2
    vanted_cols.append([''] * 22)
    vanted_cols[-1][21] = 'Time*'

    #Row 3
    vanted_cols.append([''] * 22)
    vanted_cols[-1][21] = 'Unit (Time)*'

    #Row 4
    vanted_cols.append([''] * 22)
    vanted_cols[-1][2] = 'Important Info'
    vanted_cols[-1][3] = '- Fields with a * are optional'
    vanted_cols[-1][4] = '- Yellow cells allow input'
    vanted_cols[-1][5] = '** These cells must contain numbers as 1, 2, 3, ...'
    vanted_cols[-1][6] = '*** These cells must correlate to the numbers in **'
    vanted_cols[-1][7] = '- The Experiment Name must be unique in the whole database'
    vanted_cols[-1][19] = 'Substance'
    vanted_cols[-1][20] = 'Meas.-Tool*'
    vanted_cols[-1][21] = 'Unit'

    #Rows 5-10
    vanted_cols.append([''] * 22)
    vanted_cols.append([''] * 22)
    vanted_cols.append([''] * 22)
    vanted_cols.append([''] * 22)
    vanted_cols.append([''] * 22)
    vanted_cols.append([''] * 22)
    vanted_cols[10][2] = 'Internal Info'
    vanted_cols[10][3] = 'v1.2'

    #Rows 11-...
    if len(all_states) > 11:
        for i in range(11, len(all_states)):
            vanted_cols[i] = [''] * 22

    #Special rows identified by name
    vanted_row_name_dict = OrderedDict()
    vanted_row_name_dict['Plants/Genotypes**'] = 10
    vanted_row_name_dict['Species'] = 11
    vanted_row_name_dict['Genotype'] = 13

    return vanted_cols, vanted_row_name_dict

def genefam_nsaf_analysis(prot_state_dict, prot_postnovo_df_dict, prot_count_dict):
    '''
    Calculate NSAF values for Gene Family metaproteins
    '''

    #SAF for each Gene Family metaprotein in a sample
    prot_genefam_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #NSAF for each Gene Family metaprotein in a sample
    prot_genefam_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )

    for prot_name, postnovo_df in prot_postnovo_df_dict.items():
        #Peptides with Gene Family assignment
        genefam_df = postnovo_df[pd.notnull(postnovo_df['protein'])]
        genefam_df.rename(dict([('protein', 'genefam')]), inplace=True)

        #Record SAF for each Gene Family -- multiple Gene Families can be attributed to a peptide
        genefam_saf_dict = prot_genefam_saf_dict[prot_name]
        for genefam_entry, saf in zip(
            genefam_df['genefam'].tolist(), genefam_df['saf'].tolist()
        ):
            genefams = genefam_entry.split(',')
            for genefam in genefams:
                try:
                    genefam_saf_dict[genefam] += saf
                except KeyError:
                    genefam_saf_dict[genefam] = saf

    #Calculate NSAF by dividing each SAF by sample PSM count
    for prot_name, genefam_saf_dict in prot_genefam_saf_dict.items():
        genefam_nsaf_dict = prot_genefam_nsaf_dict[prot_name]
        prot_count = prot_count_dict[prot_name]

        for genefam, saf in genefam_saf_dict.items():
            nsaf = saf / prot_count
            genefam_nsaf_dict[genefam] = nsaf

    #Write tables of NSAF values for multivariate analysis
    nsaf_data_dir = os.path.join(out_dir, 'nsaf_data')
    if not os.path.exists(nsaf_data_dir):
        os.mkdir(nsaf_data_dir)

    for i, t in enumerate(prot_state_dict.items()):
        prot_name = t[0]
        state = t[1]
        genefam_nsaf_f = 'GENEFAM_NSAF.samples.tsv'
        genefam_nsaf_fp = os.path.join(nsaf_data_dir, genefam_nsaf_f)

        genefam_nsaf_df = pd.DataFrame(
            list(prot_genefam_nsaf_dict[prot_name].items())
        ).set_index(0)
        genefam_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']    
        )

        if i == 0:
            full_genefam_nsaf_df = genefam_nsaf_df
        else:
            full_genefam_nsaf_df = pd.merge(
                full_genefam_nsaf_df, 
                genefam_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )

    #Sample/state headers and Gene Family indices are written along with data
    full_genefam_nsaf_df.to_csv(genefam_nsaf_fp, sep='\t')

    return

def go_nsaf_analysis(prot_state_dict, prot_postnovo_df_dict, prot_count_dict):
    '''
    Calculate NSAF values for GO term metaproteins
    '''

    #SAF for each GO metaprotein in a sample
    prot_go_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #NSAF for each GO metaprotein in a sample
    prot_go_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )

    for prot_name, postnovo_df in prot_postnovo_df_dict.items():
        #Peptides with GO assignment
        go_df = postnovo_df[pd.notnull(postnovo_df['go'])]

        #Record SAF for each GO term -- multiple GO terms can be attributed to a peptide
        go_saf_dict = prot_go_saf_dict[prot_name]
        for go_entry, saf in zip(
            go_df['go'].tolist(), go_df['saf'].tolist()
        ):
            go_terms = go_entry.split(',')
            for go in go_terms:
                try:
                    go_saf_dict[go] += saf
                except KeyError:
                    go_saf_dict[go] = saf

    #Calculate NSAF by dividing each SAF by sample PSM count
    for prot_name, go_saf_dict in prot_go_saf_dict.items():
        go_nsaf_dict = prot_go_nsaf_dict[prot_name]
        prot_count = prot_count_dict[prot_name]

        for go, saf in go_saf_dict.items():
            nsaf = saf / prot_count
            go_nsaf_dict[go] = nsaf

    #Write tables of NSAF values for multivariate analysis
    nsaf_data_dir = os.path.join(out_dir, 'nsaf_data')
    if not os.path.exists(nsaf_data_dir):
        os.mkdir(nsaf_data_dir)

    for i, t in enumerate(prot_state_dict.items()):
        prot_name = t[0]
        state = t[1]
        go_nsaf_f = 'GO_NSAF.samples.tsv'
        go_nsaf_fp = os.path.join(nsaf_data_dir, go_nsaf_f)

        go_nsaf_df = pd.DataFrame(
            list(prot_go_nsaf_dict[prot_name].items())
        ).set_index(0)
        go_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']    
        )

        if i == 0:
            full_go_nsaf_df = go_nsaf_df
        else:
            full_go_nsaf_df = pd.merge(
                full_go_nsaf_df, 
                go_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )

    #Sample/state headers and GO indices are written along with data
    full_go_nsaf_df.to_csv(go_nsaf_fp, sep='\t')

    return

def genefam_descrip_nsaf_analysis(
    prot_state_dict, prot_postnovo_df_dict, prot_count_dict, prot_sys_postnovo_df_dict
):
    '''
    First, calculate NSAF values of manually curated Gene Family and Description metaproteins
    Second, calculate NSAF values of manually defined functional Groups
    '''

    #Each metaprotein will have an ID that is either a Gene Family or Description
    #SAF for each metaprotein in a sample
    prot_id_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #SAF for each Group metaprotein in a sample
    prot_group_saf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #NSAF for each Gene Family metaprotein in a sample
    prot_id_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )
    #NSAF for each Group metaprotein in a sample
    prot_group_nsaf_dict = OrderedDict(
        [(prot_name, OrderedDict()) for prot_name in prot_state_dict]
    )

    for prot_name, postnovo_df in prot_sys_postnovo_df_dict.items():
        postnovo_df = postnovo_df[
            pd.notnull(postnovo_df['genefam']) | pd.notnull(postnovo_df['descrip'])
        ]
        id_saf_dict = prot_id_saf_dict[prot_name]
        group_saf_dict = prot_group_saf_dict[prot_name]
        for genefam, descrip, group, saf in zip(
            postnovo_df['genefam'].tolist(), 
            postnovo_df['descrip'].tolist(), 
            postnovo_df['group'].tolist(), 
            postnovo_df['saf'].tolist()
        ):
            if genefam == '':
                try:
                    id_saf_dict[descrip] += saf
                except KeyError:
                    id_saf_dict[descrip] = saf
            else:
                try:
                    id_saf_dict[genefam] += saf
                except KeyError:
                    id_saf_dict[genefam] = saf
            if group != '':
                try:
                    group_saf_dict[group] += saf
                except KeyError:
                    group_saf_dict[group] = saf

    #Calculate NSAF by dividing each SAF by sample PSM count
    for prot_name, id_saf_dict in prot_id_saf_dict.items():
        group_saf_dict = prot_group_saf_dict[prot_name]
        id_nsaf_dict = prot_id_nsaf_dict[prot_name]
        group_nsaf_dict = prot_group_nsaf_dict[prot_name]
        prot_count = prot_count_dict[prot_name]

        for id, saf in id_saf_dict.items():
            nsaf = saf / prot_count
            id_nsaf_dict[id] = nsaf

        for group, saf in group_saf_dict.items():
            nsaf = saf / prot_count
            group_nsaf_dict[group] = nsaf

    #Write tables of NSAF values for multivariate analysis
    nsaf_data_dir = os.path.join(out_dir, 'nsaf_data')
    if not os.path.exists(nsaf_data_dir):
        os.mkdir(nsaf_data_dir)

    for i, t in enumerate(prot_state_dict.items()):
        prot_name = t[0]
        state = t[1]
        id_nsaf_f = 'GENEFAM_DESCRIP_NSAF.samples.tsv'
        group_nsaf_f = 'MANUAL_GROUP_NSAF.samples.tsv'
        id_nsaf_fp = os.path.join(nsaf_data_dir, id_nsaf_f)
        group_nsaf_fp = os.path.join(nsaf_data_dir, group_nsaf_f)

        id_nsaf_df = pd.DataFrame(
            list(prot_id_nsaf_dict[prot_name].items())
        ).set_index(0)
        id_nsaf_df.columns = pd.MultiIndex.from_arrays(
            [[state], [prot_name]], names=['State', 'Sample']    
        )

        if i == 0:
            full_id_nsaf_df = id_nsaf_df
            full_group_nsaf_df = group_nsaf_df
        else:
            full_id_nsaf_df = pd.merge(
                full_id_nsaf_df, 
                id_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )
            full_group_nsaf_df = pd.merge(
                full_group_nsaf_df, 
                group_nsaf_df, 
                how='outer', 
                left_index=True, 
                right_index=True
            )

    #Sample/state headers and Gene Family indices are written along with data
    full_id_nsaf_df.to_csv(id_nsaf_fp, sep='\t')
    full_group_nsaf_df.to_csv(group_nsaf_fp, sep='\t')

    return

def assign_groups():
    '''
    Assign proteins and descriptions to groups
    '''

    for state in all_states:
        fp = os.path.join(out_dir, 'compar_table.' + state + '.tsv')
        compar_df = pd.read_csv(fp, sep='\t', header=0)
        compar_df['protein'] = compar_df['protein'].fillna('')
        compar_df = compar_df[
            (compar_df['protein'].isin(protein_group))
            | ((compar_df['protein'] == '') & (compar_df['descrip'].isin(descrip_group)))
            ]
        protein_df = pd.DataFrame(columns=['group', 'protein', 'descrip'])
        protein_df['protein'] = compar_df['protein']
        protein_df['descrip'] = compar_df['descrip']
        groups = []
        for p, d in zip(compar_df['protein'].tolist(), compar_df['descrip'].tolist()):
            if p == '':
                groups.append(descrip_group[d])
            else:
                groups.append(protein_group[p])
        protein_df['group'] = groups

        for bin in bin_names:
            bin_scores = []
            bin_df = compar_df[[bin + '_mean', bin + '_count']]
            bin_df.fillna(0, inplace=True)
            bin_df[bin + '_mean'] = bin_df[bin + '_mean'] - sig_cutoff
            protein_df[bin] = bin_df[bin + '_mean'] * bin_df[bin + '_count']

        protein_df.sort_values(['group', 'protein'], inplace=True)
        protein_df.to_csv(
            os.path.join(out_dir, 'protein_score.' + state + '.tsv'), sep='\t', index=False
            )

        group_df = protein_df.groupby('group').sum()
        score_df = group_df.div(group_df.max(0), axis=1)
        score_df.to_csv(
            os.path.join(out_dir, 'bin.group_score.' + state + '.tsv'), sep='\t'
            )

    return score_df

if __name__ == '__main__':
    main()